\chapter{\Large Introduction}
\label{introchap}

Large scale systems consisting of many interacting subsystems are increasingly common, with applications such as computer networks, smart grids, and robotic sensor networks. The objectives for these applications are often complex; even centralized methods must make tradeoffs between performance and speed in order to optimize these non-convex, nonlinear systems. However, due to inherent limitations in computation, communication, or sensing, large multi-agent systems are often controlled in a distributed fashion. Here, individual agents, or subsystems, must make decisions based on local, often incomplete information, potentially exacerbating tradeoffs between performance and speed. Furthermore, the distributed nature of these multi-agent systems may introduce vulnerabilities to adversarial manipulation: by influencing small subsets of agents, a malicious agent may be able to degrade a system's overall performance.%Furthermore, the distributed nature of these systems could create vulnerability to adversarial manipulation: if an adversary can influence a small subset of agents, the resulting effects may cascade throughout the system, ultimately degrading overall performance.

The overarching goals of this dissertation are to (1) characterize tradeoffs between speed, performance, vulnerability, and information available to agents in distributed control systems, and (2) design algorithms with desirable guarantees in these aspects. %Additionally, I aim to further explore the applicability of game theoretic control laws in physical systems.
In order to address these goals, this dissertation focuses specifically on the following questions:
%I continue to investigate in order to address these research goals are:

%\todo[inline]{try and fit these questions on the first page}

\begin{itemize}[leftmargin=*]
\item \textbf{When is fast convergence to near-optimal behavior possible in a distributed system?} (Chapter~\ref{ch2})
\item\textbf{Can agents in a distributed system learn near-optimal correlated behavior despite severely limited information about one another's behavior?} (Chapter~\ref{ch3})
\item\textbf{How does the structure of agent interaction impact the system's vulnerability to adversarial manipulation?} (Chapter~\ref{ch4})
\end{itemize}


\section{Informal statement of contributions}

Agents' control laws are a crucial component of any multi-agent system. They dictate how individual agents process locally available information to make decisions. Factors that determine the quality of a control law include informational dependencies, asymptotic performance guarantees, convergence rates, and resilience to adversarial influence.

Game theory has recently emerged as a framework for assigning agents' local control laws in a distributed system \cite{Marden2008, Zhu2009, Goto2010, Staudigle2012, Fox2010, Lasaulce2011, Alpcan2010, Han2012, MacKenzie2006, Menache2011}. Here, each agent is assigned a {\it utility function}\footnote{The terms ``utility" and ``payoff" will refer to individual agents' utility functions, whereas ``objective" or ``welfare" will refer to the system level objective function.} and a {\it learning rule}.\footnote{The terms ``learning rule," ``revision strategy," and ``decision making strategy" will all refer to individual agents' learning rules.} Significant research has been directed at deriving distributed utility functions and learning rules that possess desirable performance guarantees and enable agents to make decisions based on limited information.


%This dissertation focuses on game theoretic distributed control methods, which have recently gained traction as a design tool for assigning decision making rules in multi-agent systems\cite{Marden2008, Zhu2009, Goto2010, Staudigl2012, Fox2010}.  Here, agents are assigned (1) {\it utility functions}\footnote{The terms ``utility" and ``payoff" will refer to individual agents' utility functions, whereas ``objective" or ``welfare" will refer to the system level objective function.} and (2) {\it learning rules}\footnote{The terms ``learning rule," ``revision strategy," and ``decision making strategy" will all refer to individual agents' learning rules.}, which together comprise a distributed control law. 

%\todo[inline]{see wording in CCE chapter (commented out). Sounds a little better maybe}

\noindent{\it Utility functions: what should agents optimize?}An agent's local utility function dictates {\it what} it should seek to optimize. Effective utility functions are aligned with the global objective. This means that when the system is near a globally optimal configuration, individual agents are performing well with respect to their local utility functions, given others' collective behavior.  An agent's utility function typically depends on its own action choice and on the actions of others. Utilities are frequently designed to be interdependent in this way to reflect the fact that an agent's best action with respect to the system objective function depends on others' behavior. For example, if a subset of agents were to fail, changes to the the remaining agents' utilities can motivate them to adjust their behavior accordingly.

However, interdependence in agents' utility functions adds complexity to the optimization problem. Individual agents do not have full control over the function they aim to optimize. Therefore, if agents simply optimize their utility functions, this may lead to undesirable collective behavior, as we will show in Example~\ref{}. \todo[inline]{add an example here} 



Local utility functions typically align with the global objective: when collective behavior is near-optimal with respect to the system level objective function, then each agent's  action should also be near-optimal with respect to it's local utility function, {\it given other agents' current behavior}. Local utility functions are often highly coupled, i.e., an agent's utility typically depends not only on its own action, but also on other agents' actions. This coupling enables an often desirable aspect of behavior in a multi-agent system: agents should respond to one another's actions. For example, if some subset of agents were to fail, this would impact the remaining agents' utility functions, providing the impetus for these remaining agents to pick up the slack


Desired performance guarantees frequently leads to highly coupled agent utility functions, e.g., if some subset of agents fail, this should lead to a utility decrease in the remaining agents



, i.e., when the system as a whole is performing optimally, then each agent should also be performing optimally within


, and typically depends on the agent's own actions and on locally available information about other agents' behavior and the surrounding environment. 


%The structure of system level objectives often leads to highly coupled agent utilities. This means that agents do not have full control over the quantity they seek to optimize, i.e., agents are ``optimizing among optimizers." %\todo[inline]{Find a quote from Jeff to put here. And make it sound a little nicer. Maybe reference a review of various objectives (if it's included) that will come later...Also possibly add some example utilities here.}

\noindent{\it Learning rules: } An agent's learning rule dictate {\it how} it should optimize its utility function. In some cases, it may suffice for each agent to simply choose the action which maximizes its utility function, given others' current behavior. This is know as a {\it best response} learning rule. However, in many distributed systems, this approach can lead to undesirable equilibria, as we will demonstrate in Example~\ref{}. Hence, many alternative learning rules have been studied in the literature. \todo{cite some learning rules here}. In this dissertation, we primarily focus on noisy best response rules, such as log-linear learning, which we describe in detail in Section~\ref{}.
%\todo[inline]{think about adding a list of learning rules here. or maybe just leave it for the figure. ok fine bring back the diagram thingy}

The combination of agents' utility functions and learning rules dictates system dynamics, and hence determines both (1) performance with respect to the system level objective, and (2) speed of convergence. \todo[inline]{Something about vulnerability here...}



%Here, a static game is repeated over time, and agents revise their strategies based on their objective functions and on observations of other agents' behavior.  
%\todo[inline]{add utility plus learning rule discussion. also might be good to include the chart, and the phrases ``what should agents optimize" and ``how should agents optimize"}

The following informally summarizes this dissertation's primary contributions for game theoretic distributed control, with respect to the three research questions posed above.
%Emergent collective behavior for such revision strategies has been studied extensively in the literature, e.g., fictitious play \cite{fp1,fp2,jsfp}, regret matching \cite{Hart2000}, and log-linear learning \cite{Alos-Ferrer2010, Blume1993, Shah2010}.  Although many of these learning rules have desirable asymptotic guarantees, their convergence times either remain uncharacterized or are prohibitively long \cite{Ellison2000, Kandori1993,Shah2010,Hart2010}. Characterizing convergence rates is key to determining whether a distributed algorithm is desirable for system control.

%\begin{description}[leftmargin=*]
\noindent{\it\emph{Research question: When is fast convergence to near-optimal collective behavior possible? }}

\smallskip

\noindent \textbf{Contribution: Fast convergence to near-optimal collective behavior is possible when agents revise their actions according a a mild variant of log-linear learning, provided agents' utilities are their marginal contribution to the system level objective, and heterogeneity among agents is limited.}
%\end{description}


%Emergent collective behavior for game theoretic learning rules has been studied extensively in the literature, e.g., fictitious play \cite{fp1,fp2,jsfp}, regret matching \cite{Hart2000}, and log-linear learning \cite{Alos-Ferrer2010, Blume1993, Shah2010}.  Although many of these learning rules have desirable asymptotic guarantees, their convergence times either remain uncharacterized or are prohibitively long \cite{Ellison2000, Kandori1993,Shah2010,Hart2010}. Characterizing convergence rates is key to determining whether a distributed algorithm is desirable for system control.

One well-studied method of optimizing behavior in a multi-agent system is to assign agent utilities to be their {\it marginal contribution} to the overall system objective \cite{Wolpert1999}, and have agents make decisions according to the {\it log-linear learning} rule \cite{Blume1993}. In log-linear learning, agents primarily choose utility maximizing actions, but choose suboptimally with a small probability that decreases exponentially with respect to the associated payoff loss. In the long run, marginal contribution utilities with log-linear learning dynamics spends the majority of time at the global objective function maximizer. Unfortunately, worst-case convergence times for log-linear learning are known to be exponential in the number of agents, \cite{Shah2010} often rendering this game theoretic control method impractical for large-scale distributed systems. 

However, when system heterogeneity is limited, i.e., agents can be grouped into a small number of populations according to their action sets and impact on the objective function, a variant of log-linear learning achieves improved worst-case convergence times. In Chapter~\ref{ch2}, I build on the work of \cite{Shah2010} to derive this variant, which converges in near-linear time with respect to the number of agents. 








%In my PhD research, I have addressed these questions with respect to game theoretic control laws. In the future, I would like to continue this line of research, while also investigating similar questions for alternate methods of distributed control and optimization. Additionally, I aim to further explore these questions in physical systems such as robotic sensor networks or smart grids. 

%In the game theoretic control laws studied in this work, each agent is assigned (1) a \emph{utility function}, which maps an agent's information about collective behavior to a payoff, and (2) a \emph{learning rule}, which dictates how the agent makes decisions. Here, ``utility" or ``payoff" will refer to individual agents' local utility functions, and ``objective" will refer to the overall system objective function.


%In a game theoretic control law, each agent is assigned a \emph{utility function}, and a \emph{learning rule}. An agent's utility function assigns value to its action, given others' behavior and the state of the surrounding environment. A learning rule then dictates how and when agents should revise their behavior based on their utilities. The combination of utility functions and a learning rule dictates agent dynamics, and hence determines system performance, speed, and vulnerability to manipulation.


%In ongoing work, I aim to develop and characterize similar distributed algorithms with desirable short and medium term performance guarantees, since long run steady state behavior often cannot be achieved in a reasonable amount of time. Ultimately I would like to improve the practicality of these algorithms for use in real systems; thus, I am also interested in studying their implementation in distributed systems.


%\begin{description}[leftmargin=*]
\noindent{\it \emph{Research question: Can agents learn near-optimal correlated behavior despite severely limited information about one another's behavior? }}

\smallskip

\noindent\textbf{Contribution: Following the algorithm in Chapter~\ref{ch3}, agents with limited knowledge of each other's behavior can learn a utility-maximizing correlated equilibrium.}


Significant research has been directed at deriving distributed learning rules that possess desirable asymptotic performance guarantees and convergence rates and enable agents to make decisions based on limited information. The majority of this research has focused on attaining convergence to (pure) Nash equilibria under stringent information conditions \cite{Young2009, Frihauf2012, Foster2006, Boussaton2012, Poveda2013, Gharesifard2012}. Recently, the research focus has shifted to ensuring convergence to alternate types of equilibria that often yield more efficient behavior than Nash equilibria.  In particular, results have emerged that guarantee convergence to Pareto efficient Nash equilibria \cite{Marden2009,Pradelski2012}, potential function maximizers \cite{Blume1993, Marden2012}, welfare maximizing action profiles \cite{Marden2011, Arieli2012}, and the set of correlated equilibria \cite{Hart2000,Marden2013c,Aumann1987,Foster1997}, among others.  

In most cases highlighted above, the derived algorithms guarantee (probabilistic) convergence to the specified equilibria.  However, the class of correlated equilibria has posed significant challenges with regards to this goal. Learning algorithms that converge to an efficient correlated equilibrium are desirable because optimal system behavior can often be characterized by a correlated equilibrium. Unfortunately, the aforementioned learning algorithms, such as regret matching \cite{Hart2000}, merely converge to the \emph{set} of correlated equilibria. This means that the long run behavior does not necessarily constitute -- or even approximate -- a specific correlated equilibrium at any instance of time.

%We provide a distributed learning algorithm that converges to the most efficient, i.e., welfare maximizing, correlated equilibrium.  





%Agents' average utilities can often be improved when they act according to a distribution over multiple joint actions, instead of staying fixed at a single joint action.  In many cases, the desired collective behavior constitutes a {\it coarse correlated equilibrium}. A coarse correlated equilibrium is a probability distribution over the joint action space such that no agent can improve its payoff by deviating to a fixed action \cite{Aumann1987}.  Previously, algorithms existed which converged to the {\it set} of coarse correlated equilibrium, e.g., \cite{Hart2000}, without selecting any particular equilibrium; these algorithms provided no performance guarantees. An algorithm which achieves a payoff maximizing coarse correlated equilibrium through deterministic, cyclic behavior is presented in \cite{Marden2013c}. However, predictable cyclic behavior may be undesirable in many settings, e.g., in the presence of an adversary.

In Chapter~\ref{ch3} I design and analyze an algorithm that converges to a payoff maximizing coarse correlated equilibrium when agents have no knowledge of others' behavior. Here, agents' utilities depend on collective behavior, but they have no way of evaluating the utility of alternative actions. My algorithm uses a common random signal as a coordinating entity to eventually drive agents toward the desired collective behavior. In the long run, day to day behavior is selected probabilistically according to the payoff maximizing coarse correlated equilibrium.

\todo[inline]{change I's to we's? at least make it consistent}
 
%Slow convergence rates are a major drawback to this algorithm: in practice it can take a prohibitively long time to reach the desired behavior. Hence, in my future work I seek to improve upon this algorithm's convergence rates and derive a variant which is more practical in distributed system applications.

%\begin{description}[leftmargin=*]
\noindent{\it \emph{Research question: How does the structure of agent interaction impact a distributed system's vulnerability to adversarial manipulation?}}

\smallskip

 \noindent\textbf{Contribution: If every subset of agent interacts with sufficiently many other agents, the system is more resilient to adversarial manipulation.}


Agents in a distributed system often interact and share information according to a network. The structure of this network not only has an impact on a distributed control algorithm's performance and speed, but also on its resilience to adversarial manipulation. A loosely connected network may be easier to influence, because an adversary may be able to more easily manipulate the information available to subsets of agents, thereby creating impacts that cascade throughout the system. On the other hand, a well-connected network may be more difficult to influence in this way. In Chapter~\ref{ch4}, I investigate such vulnerabilities for  {\it graphical coordination games} \cite{Ullmann1977,Cooper1999} with agents revising their actions according to log-linear learning. In this work, I provided a condition based on network structure which guarantees resilience in a graphical coordination game. 

%I am currently working to fully characterize resilient systems for various models of adversarial behavior. I also aim to extend this analysis beyond simple coordination games, to systems with significantly more complex objective functions. Additionally, I am interested in investigating vulnerabilities in real distributed systems, in order to develop mathematical models that more accurately reflect the ways an adversary may seek to impact behavior. \\

%\smallskip

%Overall, I believe that these three main research questions I have pursued throughout my Ph.D. have broad relevance in many types of distributed systems. Although the underlying control algorithms, mathematical models, and analytical tools may differ, I aim to extend my study of questions like these to broader classes of distributed systems. I would like to continue grounding my work in theory, but also wish to extend my research toward practical applications in distributed engineering systems.


\section{Technical Preliminaries}


\todo[inline]{OK, i took out the utility function + learning rule table, because I think it's distracting. The majority of my work isn't specific to resource allocation games, and doesn't do anything with utility design, so I don't want to discuss it too much. Think about this some more.}


\begin{enumerate}
    \item the setting: a distributed control system. agents, actions, objective function
    \item Our approach: game theoretic control
    \begin{enumerate}
        \item Utility function that maps local information to a utility/payoff. Give a small preview of the types of utility function we use. Appropriate design varies depending on the application. My focus was on learning algorithms, so typically I assigned utilities that made analysis simple in order to gain intuition about the question I was trying to ask. Not sure whether to say this or how to convey it....
        \item Learning rule that dictates agents' decisions. Our focus is on probabilistic learning rules. Reference Markov chains section in the appendix. Discrete time, so this yields a sequence of joint actions $a(0),a(1),\ldots.$ Small discussion that these are random variables, perhaps. Include an example (LLL) and state that the majority of the dissertation will focus on LLL, with the exception of the CCE work.
    \end{enumerate}
    \item Goal (in general): design utilities and learning rules with desired performance
    \begin{enumerate}
        \item Desired long run performance. Given parameter $\varepsilon>0$, should be able to tune learning algorithm parameters to achieve:
        $$\lim_{t\to\infty}\mathbb{E}[ W(a(t)]\geq \max_{a\in\mathcal{A}}W(a) - \varepsilon$$
        \item Fast convergence rates. Given $\delta > \varepsilon$, desired
        $$\mathbb{E}[W(a(t))]\geq \max_{a\in\mathcal{A}}W(a) - \delta$$
        for all $t\geq T$, for some ``reasonable" convergence time $T$. Discussion here that $T$ typically depends on the number of agents, action sets, etc., and can often be exponential in these parameters. Goal here is to design a learning rule that's linear. Reference an example and log-linear learning.
        \item Resilience to adversaries. Suppose an adversary is able to manipulate information available to small subsets of agents. Is it possible for limited info manipulation to cascade and create catastrophic performance failures? Goal is for it not to be.
    \end{enumerate}
\end{enumerate}


\section{Formal statement of contributions}

Relate to the goals above. State each main theorem as a contribution here. First a reminder of the three main questions.

\subsection{Fast convergence contributions}

\subsection{CCE contributions}

\subsection{Adversarial vulnerability contributions}

\todo[inline]{Add large ``future work" section. There's a lot left to do!}