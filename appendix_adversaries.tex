\chapter{Technical Preliminaries and Proofs Corresponding to Chapter~\ref{ch4}}

\subsection{Log-linear learning and its underlying Markov process}\label{a:LLL Markov}

Log-linear learning dynamics define a family of aperiodic, irreducible Markov processes, $\{\tilde{P}_\beta\}_{\beta>0},$ over state space $\mathcal{A}\times\mathcal{S}_k$ with transition probabilities parameterized by $\beta$ \cite{Blume1993}. Under our adversarial model, transition probabilities are
\begin{align}
&P_\beta(((a_i,a_{-i}), S)\to (a_i^\prime,a_{-i}),S^\prime)\nonumber\\
&\hspace{0.15in} = {1\over n}\Pr[a_i(t+1) = a_i^\prime\,|\,a_{-i}(t) = a_{-i}, S(t) = S]
\end{align}
for any $i\in N,$ $a_i\in \{\vec{x},\vec{y}\},$  $(a_i,a_{-i})\in\mathcal{A}$ and $S,S^\prime\in\mathcal{S}_k$. Here $S$ transitions to $S^\prime$ according to the specified adversarial model. If $a$ and $a^\prime\in\mathcal{A}$ differ by more than one agent's action, then $P_\beta(a\to a^\prime) = 0$. 

For each model of adversarial behavior, it is straightforward to reduce $\tilde{P}_\beta$ to a Markov chain, $P_\beta$ over state space $\mathcal{A}$.
Since $P_\beta$  is aperiodic and irreducible for any $\beta >0$, it has a unique stationary distribution, $\pi_\beta$, with $\pi_\beta P_\beta = \pi_\beta$. 

As $\beta\to\infty$, the stationary distribution, $\pi_\beta$, associated with log-linear learning converges to a unique distribution,
%\begin{equation}
$\pi := \lim_{\beta\to\infty} \pi_\beta.$  
%\end{equation}
If %$\pi(a)>0,$ then $a$ is \emph{stochastically stable}, and if 
$\pi(a) = 1,$ then joint action $a$ is \emph{strictly stochastically stable} \cite{Foster1990}.\footnote{Note that this definition of strict stochastic stability is equivalent to the definition  in the introduction.}


As $\beta\to \infty$, transition probabilities $P_\beta(a\to a^\prime)$ of  log-linear learning converge to the transition probabilities, $P(a\to a^\prime)$, of a best response process.  
Distribution $\pi$ is one of possibly multiple stationary distributions of a best response process over game $G$. %Thus, log-linear learning selects a stationary distribution of the best response process. 


\subsection{Stability in the presence of a fixed, intelligent adversary}\label{a:fixed line graph proofs}

When a fixed, intelligent adversary influences set $S$, the corresponding influenced graphical coordination game is a potential game \cite{Monderer1996} with potential function 
\begin{equation}
\Phi^S(a_i,a_{-i}) = {1\over 2}\sum_{i\in N}\left(U_i(a_i,a_{-i}) + 2\cdot\mathds{1}_{i\in S, a_i = y}\right).
\end{equation}

This implies that the stationary distribution associated with log-linear learning influenced by a fixed adversary is 
\begin{equation}\label{e:stationary distn}
\pi(a) = {\exp(\beta\cdot \Phi^S(a))\over \sum_{a^\prime\in\mathcal{A}}\exp(\beta\cdot\Phi^S(a^\prime))},
\end{equation}
for $a\in\mathcal{A}$  \cite{Blume1993}. Hence, $a\in\mathcal{A}$ is strictly stochastically stable if and only if $\Phi^S(a) >\Phi^S(a^\prime)$ for all $a^\prime\in\mathcal{A},$ $a^\prime\neq a$. %This fact may be used to prove Theorems~\ref{t:A stable all} - \ref{t:fixed line graph}.

%The proof of Theorem~\ref{t:fixed line graph} follows using \eqref{e:stationary distn}.

\noindent\emph{Proof of Theorem~\ref{t:A stable all}:}
This proof adapts Proposition 2 in \cite{Young2011} to our adversarial model. Let $\mathcal{G} = (N,E)$ and suppose $S(t) =N$ for all $t\in\N$.  Define
$(\vec{y}_T,\vec{x}_{N\setminus T})$ to be the joint action $(a_1,\ldots,a_n)$ with $T = \{i\st a_i = y\}.$
It is straightforward to show that
%\small
\begin{align}
\alpha > {|T| - d(T,N\setminus T)\over d(T,N)},\quad\forall T\subseteq N\nonumber
\end{align}
if and only if
\begin{align}
\Phi^N(\vec{x})  &= (1+\alpha)d(N,N)\nonumber \\
&> (1+\alpha)d(N\setminus T,N\setminus T) + d(T,T) + |T| &\nonumber\\
&= \Phi^N(\vec{y}_T,\vec{x}_{N\setminus T})\displaybreak[3]\label{e:last eq}
%&\iff\\
%\Phi^N(\vec{x}) &> \Phi^N( \vec{y}_S,\vec{x}_{N\setminus S}),
\end{align}
\normalsize
for all $T\subseteq N$, $R\neq \emptyset,$ implying the desired result.
\hfill\qed



\noindent\emph{Proof of Theorem~\ref{t:fixed line graph} part (a):}
Let $\mathcal{G} = (N,E)$ be a line graph influenced by an adversary with  capability $k$. Joint action $\vec{x}$ is strictly stochastically stable for all $S\subseteq N$ with $|S| = k$ if and only if
\begin{align}
\Phi^S(\vec{x}) &> \Phi^S(\vec{y}_T,\vec{x}_{N\setminus T})\nonumber\\
&\iff\nonumber\\ 
(1+\alpha) &d(N,N) \nonumber\\
&>\nonumber \\
(1+\alpha)d(N\setminus T,N\setminus &T) + d(T,T) + |S\cap T|. \label{e:lastineq}
\end{align}
for all $S\subseteq N$ with $|S| = k$ and all $T\subseteq N$, $T\neq \emptyset$.

Define $t:=|T|$, let $p$ denote the number of components in the graph $\mathcal{G}$ restricted to $T$, and let $\ell$ denote the number of components in the graph restricted to $N\setminus T$. Since $T\neq \emptyset$, we have $p\geq 1$ and $\ell \in \{p-1,p,p+1\}$. 

The case where $T = N$ implies 
\begin{equation*}
\Phi^S(\vec{x}) = (1+\alpha)(n-1) >n-1+k = \Phi^S(\vec{y}),
\end{equation*}
which holds if and only if 
%\begin{equation}\label{e:ab1}
$\alpha > {k\mathop{/}(n-1)}.$
%\end{equation}

If $T\subset N$, the graph restricted to $N\setminus T$ has at least one component, i.e., $\ell\geq 1$. Then,
\begin{align}
\Phi^S(\vec{y}_T,\vec{x}_{N\setminus T}) %&= (1+\alpha)d(N\setminus T,N\setminus T) \nonumber\\
%&\quad+ d(T,T) + |S\cap T|\nonumber\\
&= (1+\alpha)(n-t-\ell) + t-p + |S\cap T|\nonumber\\
%&\leq (1+\alpha)(n-t-1) + t-1 + |S\cap T|\nonumber\\
&\leq (1+\alpha)(n-t-1) + t-1 + \min\{k,t\}\nonumber
\end{align}
where the inequality is an equality when $T = [t]$ and $S = [k].$ Then, 
\begin{align*}
\Phi^S(\vec{y}_T,\vec{x}_{N\setminus T})&\leq (1+\alpha)(n-t-1) + t-1 + \min\{k,t\} \\
&< (1+\alpha)(n-1) \\
&= \Phi^S(\vec{x})
\end{align*}
for all $T\subset N$ if and only if 
%\begin{equation}\label{e:ab2}
$\alpha >{(k-1)\mathop{/} k},$ as desired.
%\end{equation}
%Combining \eqref{e:ab1} and \eqref{e:ab2} gives the desired result.
 \hfill\qed

%\begin{Theorem}\label{t:stabilize B}

%\end{Theorem}

%\smallskip
%
\noindent\emph{Proof of Theorem~\ref{t:fixed line graph} part (b):}
Suppose $\alpha < {k\mathop{/}(n-1)}.$ Then 
$$\Phi^S(\vec{y}) = n-1+k > (1+\alpha)(n-1) = \Phi^S(\vec{x})$$
for any $S\subseteq N$ with $|S| = k.$ Then, to show that $\vec{y}$ is stochastically stable for influenced set $S$ satisfying
$$\left|S\cap [i,i+t]\right| \leq \left\lceil {kt\over n}\right\rceil,$$
it remains to show that 
$\Phi^S(\vec{y}) > \Phi^S(\vec{y}_T,\vec{x}_{N\setminus T})$
for any $T\subset N$ with $T\neq \emptyset$ and $T\neq N.$ Suppose the graph restricted to set $T$ has $p$ components, where $p\geq 1.$ Label these components as $T_1,T_2,\ldots,T_p$ and define $t:=|T|$ and $t_i:=|T_i|$ Let $\ell$ represent the number of components in the graph restricted to $N\setminus T.$ Since $\mathcal{G}$ is the line graph, we have $\ell\in \{p-1,p,p+1\}$, and since $T\neq N$, $\ell \geq 1.$ 

For any $T\subset N$ with $T\neq N,T\neq\emptyset,$ and $0<t<n,$ %we have:
\begin{align}
&\Phi^S(\vec{y}_{T},\vec{x}_{N\setminus T})\nonumber\\ 
&\quad=(1+\alpha)(n-t-\ell) + \sum_{j=1}^p \left(t_j - 1 + |S\cap T_j|\right)  \nonumber\displaybreak[3]\\
&\quad< n-1 +k\label{e:some label}\displaybreak[3]\\
&\quad=\Phi^S(\vec{y})\nonumber
\end{align}
where \eqref{e:some label} is straightforward to verify.
\hfill\qed
%
The proofs of parts (c) and (d) follow in a similar manner to parts (a) and (b), by using the potential function $\Phi^S$ for stochastic stability analysis.



\subsection{Resistance trees for stochastic stability analysis}\label{a:resistance trees}

When graphical coordination game $G$ is influenced by a mobile adversary, it is no longer a potential game; resistance tree tools defined in this section enable us to determine stochastically stable states.

The Markov process, $P_\beta$, defined by log-linear learning dynamics over a normal form game is a \emph{regular perturbation} of a best response process. In particular, log-linear learning is a regular perturbation of the best response process defined in Appendix~\ref{a:LLL Markov}, where the size of the perturbation is parameterized by $\eps = e^{-\beta}.$ The following definitions and analysis techniques are taken from \cite{Young1993}. 

\begin{defn}[\cite{Young1993}]
A Markov process with transition matrix $M_\eps$ defined over state space $\Omega$ and parameterized by perturbation $\eps\in (0,a]$ for some $a>0$ is a \emph{regular perturbation} of the process $M_0$ if it satisfies:
\begin{enumerate}[leftmargin=1.5em]
\item $M_\eps$ is aperiodic and irreducible for all $\eps\in (0,a]$.
\item $\lim_{\eps\to 0^+}M_\eps (x,y) \to M(x,y)$ for all $x,y\in\Omega.$
\item If $M_\eps(x,y) >0$ for some $\eps \in (0,a]$ then there exists $r(x,y)$ such that 
\begin{equation}\label{e:resistance}
0<\lim_{\eps\to 0^+} {M_\eps(x,y)\over \eps^{r(x,y)}} <\infty,
\end{equation}
where $r(x,y)$ is referred to as the \emph{resistance} of transition $x\to y$.
\end{enumerate}
\end{defn}

Let Markov process $M_\eps$ be a regular perturbation of process $M_0$ over state space $\Omega$, where perturbations are parameterized by $\eps\in (0,a]$ for some $a>0.$ Define graph $G = (\Omega,E)$ to be the directed graph with $(x,y)\in E \iff M_\eps(x,y)>0$ for some $\eps\in(0,a].$ Edge $(x,y)\in E$ is weighted by the resistance $r(x,y)$ defined in \eqref{e:resistance}.

Now let $\Omega_1,\Omega_2,\ldots,\Omega_n$ denote the recurrent classes of process $M_0.$ In graph $G$, these classes satisfy:
\begin{enumerate}[leftmargin=1.5em]
\item For all $x\in \Omega$, there is a zero resistance path from $x$ to $\Omega_i$ for some $i\in \{1,2,\ldots,n\}.$
\item For all $i\in \{1,2,\ldots,n\}$ and all $x,y\in \Omega_i$ there exists a zero resistance path from $x$ to $y$ and from $y$ to $x$.
\item For all $x,y$ with $x\in \Omega_i$ for some $i\in \{1,2,\ldots,n\},$ and $y\notin \Omega_i$, $r(x,y) >0.$ %\blue{be sure and define the resistance of a zero probability transition to be $\infty$!}
\end{enumerate}
Define a second directed graph, $\mathcal{G} = (V,\mathcal{E})$, where $V = \{1,2,\ldots,n\}$ are the indices of the $n$ recurrent classes in $\Omega.$ For this graph, $(i,j)\in \mathcal{E}$ for all $i,j\in \{1,2,\ldots,n\},$ $i\neq j.$ Edge $(i,j)$ is weighted by the resistance of the lowest resistance path starting in $\Omega_i$ and ending in $\Omega_j$, i.e., 
\begin{equation}
R(i,j):=\min_{i\in \Omega_i,j\in\Omega_j}\min_{p\in \mathcal{P}(i\to j)} r(p),
\end{equation}
where $\mathcal{P}(i\to j)$ denotes the set of all simple paths in $G$ beginning at $i$ and ending at $j$. For path $p = (e_1,e_2,\ldots,e_k)$, 
\begin{equation}
r(p):= \sum_{\ell = 1}^k r(e_\ell).
\end{equation}
Let $\mathcal{T}_i$ be the set of all trees in $\mathcal{G}$ rooted at $i$, and define 
\begin{equation}
\gamma_i := \min_{t\in \mathcal{T}_i} R(t)
\end{equation}
to be the \emph{stochastic potential} of $\Omega_i,$ where the resistance of tree $t$ is the sum of the resistances (in $\mathcal{G}$) of its edges,
\begin{equation}
R(t) := \sum_{e\in t} R(e).
\end{equation}
We use the following theorem due to \cite{Young1993} in our analysis:
\begin{Theorem}[\cite{Young1993}]\label{t:resistance trees theorem}
State $x\in \Omega$ is stochastically stable if and only if $x\in \Omega_i$ where 
\begin{equation}
\gamma_i = \min_{j\in \{1,2,\ldots,n\}}\gamma_j,
\end{equation}
i.e., $x$ belongs to a recurrent class which minimizes the stochastic potential. 
Furthermore, $x$ is strictly stochastically stable if and only if $\Omega_i = \{x\}$ and 
$\gamma_i<\gamma_j,\quad\forall j\neq i.$
\end{Theorem}


\subsection{Stability in the presence of a mobile, random adversary}\label{a:mobile random}

The following lemma applies to any graphical coordination game in the presence of a mobile, random adversary with capability $k\leq n-1$. It states that a mobile random adversary decreases the resistance of transitions when an agent in $N$ changes its action from $x$ to $y$, but does not change the resistance of transitions in the opposite direction.

\begin{lemma}\label{t:mobile random}
Suppose agents in $N$ update their actions according to log-linear learning in the presence of a mobile, random adversary with capability $k$, where $1\leq k\leq n-1.$ Then the resistance of a transition where agent $i\in N$ changes its action from $x$ to $y$ is:
\begin{align}
&r((x,a_{-i})\to (y,a_{-i}) )\nonumber\\
&\quad=  \max\left\{U_i(x,a_{-i}) - U_i(y,a_{-i}) -1,0\right\}
\end{align}
and the resistance of a transition where agent $i\in N$ changes its action from $y$ to $x$ is:
\begin{align}
&r((y,a_{-i})\to (x,a_{-i}) )\nonumber\\
&\quad=  \max\left\{U_i(y,a_{-i}) - U_i(x,a_{-i}), 0\right\}.\label{e:BtoA1}
\end{align}
Here $U_i:\mathcal{A}\to\R$, defined in \eqref{e:original utility}, is the utility function for agent $i$ in the uninfluenced game, $G$.  
\end{lemma}

\noindent\emph{Proof:}
In the presence of a mobile, random agent, %the probability that agent $i\in N$ changes its action from $x$ to $y$ is:
\begin{align*}
&P_\beta \left((x,a_{-i})\to (y,a_{-i})\right) \\
&\quad= {1\over n} \left({k\over n}\cdot{\exp( \beta(U_i(y,a_{-i})+1))\over \exp( \beta (U_i(y,a_{-i})+1)) + \exp( \beta U_i(x,a_{-i}))}\right.\\
&\hspace{0.45in} + \left.{n-k\over n}\cdot{\exp( \beta U_i(y,a_{-i}))\over \exp( \beta U_i(y,a_{-i})) + \exp( \beta U_i(x,a_{-i}))}\right)
\end{align*}
Define $P_\eps\left((x,a_{-i})\to (y,a_{-i})\right)$ by substituting $\eps = e^{-\beta}$ into the above equation. It is straightforward to see that
\begin{equation*}
0<\lim_{\eps\to 0^+}{P_{\eps} \left((x,a_{-i})\to (y,a_{-i})\right)\over \eps^{  U_i(x,a_{-i}) - U_i(y,a_{-i}) -1 }}<\infty,
\end{equation*}
implying 
\begin{align*}
&r((x,a_{-i})\to (y,a_{-i}) )\nonumber\\
&\quad=  \max\left\{U_i(x,a_{-i}) - U_i(y,a_{-i}) -1,0\right\}.
\end{align*}
Equation \eqref{e:BtoA1} may be similarly verified.
\hfill\qed

%Theorem~\ref{t:mr ss} follows by using Lemma~\ref{t:mobile random} and analyzing the resistance between recurrent classes of the  underlying Markov process to identify stochastically stable states. Theorem \ref{t:intelligent} follows similarly. Their details are omitted for brevity.


\noindent\emph{Proof of Theorem~\ref{t:mr ss}: }\label{a:proof mr ss}
First we show that, for any $\alpha >0,$ $\vec{x}$ and $ \vec{y}$ are the only two recurrent classes of the unperturbed process, $P$, for the line. Then we show that, for the perturbed process, 
$R(\vec{x}, \vec{y}) < R( \vec{y},\vec{x}) \iff \alpha >1$ 
and 
$R( \vec{y},\vec{x}) < R(\vec{x}, \vec{y}) \iff \alpha <1.$
That is, when $\alpha >1$ and $\beta$ is large, the lowest resistance path from $\vec{x}$ to $ \vec{y}$ occurs with higher probability than the lowest resistance path from $ \vec{y}$ to $\vec{x}$ in $P_\beta$, and vice versa when $\alpha <1.$ 
 Combining this with Theorem~\ref{t:resistance trees theorem} proves Theorem~\ref{t:mr ss}.

\noindent\emph{Recurrent classes of $P$ for the line:} Note that, 
$P(\vec{x},a) = 0$ for all $a\in \mathcal{A}, a\neq \vec{x},$
and
$P( \vec{y},a) = 0$ for all $a\in \mathcal{A}, a\neq  \vec{y},$
implying $\vec{x}$ and $ \vec{y}$ are recurrent. To show that no other state is recurrent, we will show that, for any $a\in \mathcal{A}\setminus \{\vec{x}, \vec{y}\}$, there is a sequence of positive probability transitions in $P$ leading from $a$ to $\vec{x}$.

Let $a\in\mathcal{A}$ with $a\neq \vec{x}, \vec{y}.$ Without loss of generality, choose $i,i+1$ such that $a_i = y$ and $a_{i+1} = x.$ Denote $(a_i,a_{-i}) = a,$ and note that:
\begin{equation}\label{e:pos prob}
P((y,a_{-i})\to (x,a_{-i})) = {1\over n}\cdot{n-k\over n}>0
\end{equation}
for any $k\leq n-1$ and $\alpha >0$.
Since \eqref{e:pos prob} holds for any $a\neq \vec{x},\vec{y},$ we can construct a sequence of at most $n-1$ positive probability transitions leading to joint action $\vec{x}$. Therefore $a$ cannot be recurrent in $P.$

\noindent\emph{Resistance between recurrent classes $\vec{x}$ and $ \vec{y}$:} We will show that for all $1\leq k\leq n-1$,
\begin{align}
R( \vec{y}, \vec{x}) &= 1,\quad\forall \alpha >0, \label{e:B to A}\\
R(\vec{x}, \vec{y}) &\geq \alpha,\quad\forall\alpha >0,\label{e:A to B ineq}\\
\text{and}\quad R(\vec{x}, \vec{y})&=\alpha, \quad \forall \alpha \leq 1.\label{e:A to B eq}
\end{align}
For \eqref{e:B to A}, we have
$r( \vec{y}, (x,y,\ldots,y)) = 1,$
and $r( \vec{y},a) \geq 1$ for any $a\neq  \vec{y},$ implying that $R( \vec{y}, \vec{x}) \geq 1.$ Then, since 
$$r\left((\vec{x}_{[t]}, \vec{y}_{[t+1,n]}),(\vec{x}_{[t+1]}, \vec{y}_{[t+2,n]}) \right)= 0,$$
for any $1\leq t\leq n-1$, and 
$$r\left((\vec{x}_{[n-1]}, \vec{y}_{[n,n]}),\vec{x}\right) = 0,$$ the path 
$ \vec{y}\to(x,y,\ldots,y)\to (x,x,y,\ldots,y)\to\cdots\to  \vec{y}$
has resistance 1. Since we know $R( \vec{y}, \vec{x}) \geq 1,$ this implies that $R( \vec{y}, \vec{x}) = 1.$

Now, for \eqref{e:A to B ineq}, since $r(\vec{x}, a)\geq \alpha$
for any $a\neq \vec{x},$ this implies $R(\vec{x}, \vec{y}) \geq \alpha.$ In particular $r(\vec{x}\to (y,x,\ldots,x)) = \alpha.$ When $\alpha <1,$ 
$$r\left(( \vec{y}_{[t]},\vec{x}_{[t+1,n]}),( \vec{y}_{[t+1]},\vec{x}_{[t+2,n]})\right) = 0$$
for any $1\leq t\leq n-1$, and
$$r\left(( \vec{y}_{[n-1]},\vec{x}_{[n,n]}), \vec{y}\right) = 0,$$
implying that the path 
$\vec{x}\to (y,x,\ldots,x)\to (y,y,\ldots,x)\to\cdots\to  \vec{y}$
has resistance $\alpha$ when $\alpha \leq 1.$ Hence $R(\vec{x}, \vec{y}) = \alpha.$

Combining \eqref{e:B to A} - \eqref{e:A to B eq} with Theorem~\ref{t:resistance trees theorem} establishes Theorem~\ref{t:mr ss}.
\hfill\qed

\subsection{Stability in the presence of an intelligent, mobile agent}\label{a:intelligent proof}

Define $P^\mu_\beta$ to be the Markov process associated with log-linear learning in the presence of a mobile, intelligent adversary using policy $\mu.$

\noindent\emph{Proof of Theorem~\ref{t:intelligent} part (a):}
Let $G = (N,E)$ be the line, influenced by a mobile, intelligent adversary with capability $k = 1.$ For any policy $\mu:\mathcal{A}\to\mathcal{S} = N$, if $\alpha \neq 1,$ only $\vec{x}$ and $ \vec{y}$ are recurrent in the unperturbed process, $P^\mu$. This can be shown via an argument similar to the one used in the proof of Theorem~\ref{t:mr ss}. 

Define $\mu^\star$ as in \eqref{e:mustar 1}. We will show that, (1) in $P^{\mu^\star}_\beta$, $\vec{x}$ is stochastically stable if and only if $\alpha >1$, and $ \vec{y}$ is stochastically stable if and only if $\alpha<1$, and (2) $\mu^\star$ is optimal, i.e., if $\alpha = 1,$ $\vec{x}$ is stochastically stable for any $\mu\in M_1,$ and if $\alpha >1$, $\vec{x}$ is strictly stochastically stable for any $\mu\in M_1.$

For policy $\mu\in M_1$, let $r^{\mu}(a,a^\prime)$ denote the single transition resistance from $a$ to $a^\prime\in \mathcal{A}$ in $P^\mu_\beta,$ and let $R^\mu(a,a^\prime)$, denote the resistance of the lowest resistance path from $a$ to $a^\prime\in \mathcal{A}$. 

For any $\mu\in M_1$, we have
%\begin{align*}
$r^\mu(\vec{x}, a)\geq \alpha,\ \forall a\in\mathcal{A},a\neq \vec{x},$ and
$r^\mu( \vec{y}, a) \geq 1,\quad\forall a\in\mathcal{A}, a\neq  \vec{y}.$
Therefore
\begin{align}
R^\mu(\vec{x}\to \vec{y})\geq\alpha,\text{ and }R^\mu ( \vec{y},\vec{x})\geq 1\label{e:ab1}.%\\
%\text{and } \label{e:ba1}
\end{align}

If $\alpha <1$, the path
$\vec{x}\to(y,x,\ldots,x)\to (y,y,x,\ldots,x)\to\cdots\to  \vec{y}$
in $P^{\mu^\star}_\beta$ has total resistance $\alpha$. Equation \eqref{e:ab1} implies that
$R^{\mu^\star}(\vec{x}, \vec{y})  = \alpha < 1 \leq R^{\mu^\star}( \vec{y},\vec{x}),$
so by Theorem~\ref{t:resistance trees theorem}, $ \vec{y}$ is strictly stochastically stable in $P^{\mu^\star}.$

If $\alpha = 1,$ it is straightforward to show that both $\vec{x}$ and $ \vec{y}$ are stochastically stable in $P^{\mu^\star}_\beta$. Moreover, for any $\mu\in\mathcal{M}$, either the resistance of path 
$$ \vec{y}\to (x,y,\ldots,y)\to (x,x,y,\ldots y)\to\cdots\to \vec{x}$$
or the resistance of path 
$$ \vec{y}\to (y,\ldots,y,x)\to (y\ldots,y,x,x)\to\cdots\to \vec{x}$$
is 1, and hence it is impossible to find a policy with $R^\mu(\vec{x}, \vec{y}) < R^\mu( \vec{y},\vec{x}).$

If $\alpha >1$, similar arguments show that $R^\mu( \vec{y},\vec{x}) = 1$ for any $\mu\in M_k.$ Combining this with \eqref{e:ab1} implies that $\vec{x}$ is stochastically stable for any $P^\mu_\beta,$ $\mu\in\mathcal{M}$.
\hfill\qed

\noindent\emph{Proof of (b):}
Again let $G = (N,E)$ be the line, and suppose the adversary has capability $k$ with $2\leq k\leq n-1.$ We will show that, for a policy $\mu^\star$ which satisfies Conditions 1 - 3 of Theorem~\ref{t:intelligent}, $\vec{x}$ is strictly stochastically stable in $P^{\mu^\star}$ if and only if $\alpha > {n\over n-1}$, and $ \vec{y}$ is strictly stochastically stable if and only if $\alpha < {n\over n-1}.$ Since this is the same bound on $\alpha$ when we have an adversary with capability $n$, from Theorem~\ref{t:fixed line graph} part (a), this also proves that policy $\mu^\star$ is optimal, i.e., no other policy can stabilize a state $a\in \mathcal{A}$ with $a_i =  \vec{y}$ for some $i\in N$ when $\alpha > {n\over n-1}.$

First note that only $ \vec{y}$ is recurrent in $P^{\mu^\star}$ when $\alpha \leq 1$, and hence $ \vec{y}$ is strictly stochastically stable in $P^{\mu^\star}_\beta$.

Now assume $\alpha > 1.$ Again, it is straightforward to verify that only $\vec{x}$ and $ \vec{y}$ are recurrent in $P^{\mu^\star}$. Note that
%\begin{align}
$r(\vec{x}\to a) \geq \alpha, \forall a\neq \vec{x},$  and $ r( \vec{y}\to a) =2, \forall a\neq  \vec{y}.$
%\text{and } %\label{e:res 2}
%\end{align}
Moreover, the path 
$\vec{x}\to(y,x,\ldots,x)\to (y,y,x,\ldots,x)\to\cdots\to  \vec{y}$
has total resistance $\alpha+(n-2)(\alpha-1)$ in $P^{\mu^\star}_\beta$. 

It is straightforward to verify that this is the least resistance path from $\vec{x}$ to $ \vec{y}$ in $P^{\mu^\star}_{\beta}$, implying 
%\begin{equation}\label{e:resistance path 1}
$R(\vec{x}, \vec{y})= \alpha+(n-2)(\alpha-1).$
%\end{equation}
The path
$ \vec{y}\to(x,y,\ldots,y)\to (x,x,y,\ldots,y)\to\cdots\to \vec{x}$
has resistance 2; hence
%\begin{equation}\label{e:resistance path 2}
$R( \vec{y}\to \vec{x}) = 2.$
%\end{equation}
\hfill\qed




