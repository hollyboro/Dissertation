\chapter{Technical Preliminaries and Proofs Corresponding to Chapter~\ref{ch3}}

\section{Proof of Theorem~\ref{t:main theorem}}

The formulation of the decision making process defined in Section~\ref{s:learning algorithm} ensures that the evolution of the agents' states over the periods $\{0, 1, 2, \dots\}$ can be represented as a finite ergodic Markov chain over the state space
%
\begin{equation}
X = X_1 \times \dots \times X_n
\end{equation}
%
where $X_i = S_i\times \{C,D\}$ denotes the set of possible states of agent $i$.  Let $P^\eps$ denote this Markov chain for some $\eps > 0$, and $\delta = \eps$.  Proving Theorem~\ref{t:main theorem} requires characterizing the stationary distribution of the family of Markov chains $\{P^\eps\}_{\eps > 0}$ for all sufficiently small $\eps$.  We employ the theory of resistance trees for regular perturbed processes, introduced in \cite{Young1993}, to accomplish this task.  We begin by reviewing this theory and then proceed with the proof of Theorem~\ref{t:main theorem}.


\todo[inline]{combine resistance trees background here with background from adversary paper}

\subsection{Background: Resistance Trees}\label{a:resistance trees}

Define $P^0$ as the transition matrix for some nominal Markov process, and let $P^{\eps}$ be a perturbed version of this nominal process where the size of the perturbation is $\eps > 0$.  Throughout this paper, we focus on the following class of Markov chains.  

\begin{defn}\label{d:RPP}%{\cite{Young1993}}
A family of Markov chains defined over a finite state space $X$, whose transition matrices are denoted by $\{P^\eps\}_{\eps > 0}$, is called a \emph{regular perturbed process} of a nominal process $P^0$ if the following conditions are satisfied for all $x,x^\prime\in X$:
%
\begin{enumerate}
\item There exists a constant $c>0$ such that $P^\eps$ is aperiodic and irreducible for all $\eps \in (0,c]$.
\item $\lim_{\eps\to 0} P^{\eps}_{x \rightarrow x'}= P^0_{x \rightarrow x'}$.% where $P^{\eps}_{x \rightarrow x'}$ denotes the probability of transitioning from $x$ to $x'$ given the Markov chain $P^{\eps}$.  
\item If $P^\eps_{x \rightarrow x'} > 0$ for some $\eps>0$, then there exists a constant $r(x \to x') \geq 0$ such that 
\begin{equation}\label{e:RPP bounds}
0<\lim_{\eps\to 0}
\frac{P^\eps_{x \to x'}}
{\eps^{r(x \to x')}}<\infty.
\end{equation}
The constant $r(x \to x')$ is referred to as the \emph{resistance} of the transition $x \to x'.$
%
\end{enumerate}
\end{defn}

For any $\eps > 0$, let $\mu^{\eps} = \{\mu^{\eps}_x \}_{x \in X}  \in \Delta(X)$ denote the unique stationary distribution associated with $P^{\eps}$.  The theory of resistance trees presented in \cite{Young1993} provides efficient mechanisms for computing the support of the limiting stationary distribution, i.e., $\lim_{\eps \rightarrow 0^+} \mu^{\eps}$, commonly referred to as the stochastically stable states.  

\begin{defn}\label{d:ss}
A state $x \in X$ is \emph{stochastically stable} \cite{Foster1990} if $\lim_{\eps\to 0^+}\mu_x^\eps>0$, where $\mu^\eps$ is the stationary distribution corresponding to $P^\eps.$
\end{defn}

In this paper, we adopt the technique provided in \cite{Young1993} for identifying the stochastically stable states through a graph theoretic analysis over the recurrent classes of the unperturbed process $P^0$.  To that end, let $Y_0, Y_1, \dots, Y_m$ denote the recurrent classes of $P^0$.  Define ${\cal P}_{i j}$ to be the set of all paths connecting $Y_i$ to $Y_j$, i.e., a path $p \in {\cal P}_{i j}$ is of the form $p=\{(x_1, x_2), (x_2, x_3), \dots, (x_{k-1}, x_k)\}$ where $x_1 \in Y_i$ and $x_k \in Y_j$.  The resistance associated with transitioning from $Y_i$ to $Y_j$ is defined as 
%
\begin{equation}\label{eq:321}
r(Y_i , Y_j) = \min_{p \in {\cal P}_{i j}} \sum_{(x,x') \in p} r(x,x'). 
\end{equation}

The recurrent classes $Y_0,Y_1,\ldots,Y_m$ satisfy the following properties: (i) there is a zero resistance path, i.e., a sequence of transitions each with zero resistance, from any state $x \in X$ to at least one state $y$ in one of the recurrent classes; (ii) for any recurrent class $Y_i$ and any states $y_i,y_i' \in Y_i$, there is a zero resistance path from $y_i$ to $y_i'$; and (iii) for any state $y_i \in Y_i$ and $y_j \in Y_j$, $Y_i \neq Y_j$, any path from $y_i$ to $y_j$ has strictly positive resistance.  

The first step in identifying the stochastically stable states is to identify the resistance between the various recurrent classes.  %
The second step focuses on analyzing spanning trees of the weighted, directed graph $\mathcal{G}$ whose vertices are recurrent classes of the process $P^0,$ and whose edge weights are given by the resistances between classes in (\ref{eq:321}). Denote $\mathcal{T}_{i}$ to be the set of all spanning trees of $\mathcal{G}$ rooted at recurrent class $Y_i$. Next, we compute the stochastic potential of each recurrent class which is defined as follows:

%
\begin{defn}
The \emph{stochastic potential} of recurrent class $Y_i$ is 
\begin{equation*}
\gamma(Y_i) = \min_{{\cal T} \in \mathcal{T}_{i}} \sum_{(Y, Y')\in {\cal T}} r(Y,Y')
\end{equation*}
\end{defn}
%
The following theorem characterizes the recurrent classes that are stochastically stable.

\begin{Theorem}[\cite{Young1993}] \label{t:Young Theorem}
Let $P^0$ be the transition matrix for a stationary Markov process over the finite state space $X$ with recurrent communication classes $Y_1,\ldots,Y_m$. For each $\eps > 0$, let $P^\eps$ be a regular perturbation of $P^0$ with a unique stationary distribution $\mu^\eps$. Then:
\begin{enumerate}
\item  As $\eps\to 0$, $\mu^\eps$ converges to a stationary distribution $\mu^0$ of $P^0.$
\item A state $x \in X$ is stochastically stable if and only if $x$ is contained in a recurrent class $Y_j$ that minimizes $\gamma(Y_j).$
\end{enumerate}
\end{Theorem}


\subsection{Proof of Theorem~\ref{t:main theorem}}\label{s:proof}

We begin by restating the main results associated with Theorem~\ref{t:main theorem} (setting $\delta = \eps$) using the terminology defined in the previous section.  
%
\begin{itemize}%[leftmargin = .3cm]
%
\item If $q(S) \cap {\rm CCE} \neq \emptyset$, then a state $x=\{x_i = [s_i, m_i] \}_{i \in N}$ is stochastically stable if and only if (i) $m_i = C$ for all $i \in N$ and (ii) the strategy profile $s = (s_1, \dots, s_n)$ constitutes an efficient coarse correlated equilibrium, i.e., 
%
\begin{equation}\label{e:SS1}
q(s) \in \underset{ q \in q(S) \cap {\rm CCE}}{\arg \max} \ \sum_{i \in N} \sum_{a \in \aee} \ U_i(a) q^a. 
\end{equation}
%
%
\item If $q(S) \cap {\rm CCE} = \emptyset$, then a state $x=\{x_i = [s_i, m_i] \}_{i \in N}$ is stochastically stable if and only if (i) $m_i = C$ for all $i \in N$ and (ii) the strategy profile $s = (s_1, \dots, s_n)$ constitutes an efficient action profile, i.e., 
%
\begin{equation}\label{e:SS2} 
q(s) \in \underset{ q \in q(S)}{\arg \max} \ \sum_{i \in N} \sum_{a \in \aee} \ U_i(a) q^a. 
\end{equation} 
%
\end{itemize}

\noindent For convenience, and with an abuse of notation, define
\begin{equation}
U_i(s) := \sum_{a\in \mathcal{A}}U_i(a)q^a(s)
\end{equation}
to be agent $i$'s expected utility with respect to distribution $q(s)$, where $s\in S.$

The proof of Theorem~\ref{t:main theorem} will consist of the following steps:
%
\begin{enumerate}[(i)]
\item Define the unperturbed process, $P^0$.
\item Determine the recurrent classes of process $P^0$.   
\item Establish transition probabilities of process $P^\eps$.
\item Determine the stochastically stable states of $P^\eps$ using Theorem~\ref{t:Young Theorem}.
\end{enumerate}

\vspace{.2cm}
\noindent \emph{Part 1:  Defining the unperturbed process}
\vspace{.2cm}

The unperturbed process $P^0$ is effectively the process identified in Section~\ref{s:learning algorithm} where $\eps = 0$.  Rather than dictate the entire process as done previously, here we highlight the main attributes of the unperturbed process that may not be obvious upon initial inspection.  

\begin{itemize}%S[leftmargin = .3cm]
%
\item If agent $i$ is content, i.e., $x_i = [s_i^b, C]$, the trial action is $s_i^t = s_i^b$ with probability $1$. Otherwise, if agent $i$ is discontent, the trial action is selected according to (\ref{eq:981}). 
%
\item The baseline utility $u_i^b$ in (\ref{e:baseline payoff}) associated with joint baseline strategy $s^b$ is now of the form
%
\begin{eqnarray}\label{eq:980}
u_i^b = U_i(s^b) .
\end{eqnarray}
%
This results from invoking the law of large numbers since $\p = \lceil 1/\eps^{nc+1}\rceil$.  The trial utility $u_i^t$ and acceptance utility $u_i^a$ are also of the same form.  
%
\item A content player will only become discontent if $u_i^a < u_i^b$ where associated payoffs are computed according to (\ref{eq:980}).
%
\end{itemize}

\vspace{.2cm}
\noindent \emph{Part 2: Recurrent classes of the unperturbed process}
\vspace{.2cm}

The second part of the proof analyzes the recurrent classes of the unperturbed process $P^0$ defined above.  The following lemma identifies the recurrent classes of $P^0$.  

\begin{lemma}\label{l:recurrent}
A state $x = (x_1,x_2,\ldots,x_n)\in X$ belongs to a recurrent class of the unperturbed process $P^0$ if and only if the state $x$ fits into one of following two forms:
%
\begin{itemize}%[leftmargin = .3cm]
%
\item \emph{Form \#1:} The state for each agent $i \in N$ is of the form $x_i = \left[s_i^b,C\right]$ where $s_i^b \in S_i$. Each state of this form comprises a distinct recurrent classes.  We represent the set of states of this form by $C^0$.
%
\item \emph{Form \#2:} The state for each agent $i \in N$ is of the form $x_i = \left[s_i^b,D\right]$  where $s_i^b \in S_i$. All states of this form comprise a single recurrent class, represented by $D^0$. 
%
\end{itemize}
%
\end{lemma}
%
\vspace{.2cm}
%
\begin{proof}
%
We begin by showing that any state $x \in C^0$ is a recurrent class of the unperturbed process.  According to $P^0$, if the system reaches state $x$, then it remains at $x$ with certainty for all future time. Hence, each $x\in C^0$ is a recurrent class of $P^0.$  Next, we show that $D^0$ constitutes a single recurrent class.  Consider any two states $x,y\in D^0$.  According to the unperturbed process, $P^0$, the probability of transitioning from $x$ to $y$ is strictly positive $\left(\geq \prod_{i\in N}1/|S_i|\right)$; hence, the resistance of the transition $x \rightarrow y$ is $0$.  Further note that the probability of transitioning to any state not in $D^0$ is zero. Hence, $D^0$ forms a single recurrent class of $P^0$. 
%


The last part of the proof involves proving that any state $ x = \{[s_i^b, m_i]\}_{i \in N} \notin C^0 \cup D^0$ is not recurrent in $P^0$.  Since $x\notin {C^0\cup D^0}$, it consists of both content and discontent players.  Denote the set of discontent players by $J= \{i\in N\st m_i = D\} \neq \emptyset$.  We will show that the discontent players $J$ will play a sequence of strategies with positive probability that drives at least one content player to become discontent.  Repeating this argument at most $n$ times shows that any state $x$ of the above form will eventually transition to the all discontent state, proving that $x$ is not recurrent.  

To that end, let $x(1) = x$ be the state at the beginning of the $1$-st period.  According to the unperturbed process $P^0$, each discontent player randomly selects a strategy $s_i \in S_i$ which becomes part of the player's state at the ensuing stage.  Suppose each discontent agent selects a trial strategy $s_i = (a_i^1, \dots, a_i^w) \in {\cal A}_i^w \subset S_i$ during the $1$-st period, i.e., the discontent players select strategies of the finest granularization. Note that each agent selects a strategy with probability $\geq {1\mathop{/}|S_i|}.$  Here, the trial payoff for each player $i \in N$ associated with the joint strategies $s = (\{s_i^b\}_{i \notin J}, \{s_i\}_{i \in J})$ is 
%ï¿½
\begin{eqnarray}
u_i^t(s) &=&  \int_{0}^{1} U_i(s(z)) dz \\
&=&  \frac{1}{w} U_i({a}) + \int_{w}^{1} U_i(s'(z)) dz, 
\end{eqnarray}
%
for some ${a} \in {\cal A}$ as $s_i(z) = s_i(z')$ for any $z,z' \in [0,1/w]$ for any agent $i \in N$.  If  $u_i^t < u_i^b$ for any any agent $i \notin J$, agent $i$ becomes discontent in the next stage and we are done.  

For the remainder of the proof suppose $u_i^t(s) \geq u_i^b(s^b)$ for all agents $i \notin J$. This implies all agents $N \setminus J$ will be content at the beginning of the second stage.  By interdependence, there exists a collective action $\tilde{a}_J \in \prod_{j \in J} {\cal A}_j$ and an agent $i \notin J$ such that $U_i(a) \neq U_i(\tilde{a}_J, a_{N\setminus J})$.   Suppose each discontent agent selects a trial strategy $s_i' = (\tilde{a}_i^1, a_i^2, \dots, a_i^w) \in {\cal A}_i^w \subset S_i$ during the second period, i.e., only the first component of the strategy changed.  The trial payoff for each player $i \in N$ associated with the joint strategies $s' = (\{s_i^b\}_{i \notin J}, \{s_i'\}_{i \in J})$ is 
%
\begin{eqnarray*}
u_i^t(s') &=&  \int_{0}^{1} U_i(s'(z)) dz \\
&=&  \frac{1}{w} U_i(\tilde{a}_J, a_{N\setminus J}) + \int_{w}^{1} U_i(s'(z)) dz \\ 
&\neq&  u_i^t(s)
\end{eqnarray*}
%f
If $u_i^t(s') < u_i^t(s)$, agent $i$ will become discontent at the ensuing stage and we are done.  Otherwise, %if $u_i^t(s') > u_i^t(s)$, 
agent $i$ will stay content at the ensuing stage.  However, if each discontent agent selects a trial strategy $s_i'' = (a_i^1, a_i^2, \dots, a_i^w) \in {\cal A}_i^w \subset S_i$ during the third period, we know $u_i^t(s'') < u_i^t(s')$, where $s'' = (\{s_i^b\}_{i \notin J}, \{s_i''\}_{i \in J})$.  Hence, agent $i$ will become discontent at the beginning of period $4$. This argument can be repeated at most $n$ times, completing the proof.  
%
\end{proof}

\vspace{.2cm}
\noindent \emph{Part 3:  Transition probabilities of process $P^\eps$}
\vspace{.2cm}

Here, we establish the transition probability $P^\eps_{x\to x^+}$ for a pair of arbitrary states, $x,x^+\in X.$ %Then we show that this transition probability satisfies \eqref{e:RPP bounds} in the case where all agents transition from discontent to content; all other transitions follow in a similar manner. 
 Let $x_i = [s_i,m_i]$, $x_i ^+= [s_i^+,m_i^+]$ for $i\in N,$  $s = (s_1,s_2,\ldots,s_n),$  and $s^+ = (s_1^+,s_2^+,\ldots,s_n^+).$ Then,
%\small
\begin{align}
P^\eps_{x\to x^+} 
&=\sum_{\tilde{s}s^t\in S}\sum_{\tilde{s}^a\in S}\biggl(\Pr[x^+\given s^t = \tilde{s}^t,\, s^a = \tilde{s}^a]\nonumber\\
&\hspace{.4in}\times\Pr[s^a = \tilde{s}^a \given s^t = \tilde{s}^t]\Pr[s^t = \tilde{s}^t]\biggr).\label{e:prob1}
\end{align}
%\normalsize
Note that the strategy selections and state transitions are conditioned on state $x$; for notational brevity we do not explicitly write this dependence. Here, $s^t$ and $s^a$ represent the joint trial and acceptance strategies during the period before the transition to $x^+.$. The double summation in \eqref{e:prob1} is over all possible trial actions, $\tilde{s}^t\in S$, and acceptance strategies, $\tilde{s}^a\in S$. However, recall from \eqref{e:state trans1a} - \eqref{e:D state trans} that, when transitioning from $x$ to $x^+$, not all strategies can serve as intermediate trial and acceptance strategies. In particular, transitioning to state $x^+$ requires that $s^a = s^+;$ hence if $\tilde{s}^a\neq s^+,$ then
$\Pr[x^+\given s^t = \tilde{s}^t,\, s^a = \tilde{s}^a]=0,$ 
so we can rewrite \eqref{e:prob1} as:
\begin{align}
P^\eps_{x\to x^+}
%&\quad=\sum_{s\in S}\biggl(\Pr[x(k+1) = y\given s^t = s,\, s^a = s^y,\,x(k)=x]\nonumber\\
%&\hspace{1in} \times\Pr[s^t = s, s^a = s^y\given x(k) = x]\biggr)\nonumber\\
& = \sum_{\tilde{s}^t\in S}\biggl(\Pr[x^+\given s^t = \tilde{s}^t,\, s^a = s^+]\nonumber\\
&\hspace{.3in} \times\Pr[ s^a = s^+\given s^t = \tilde{s}^t]\Pr[ s^t = \tilde{s}^t]\biggr)\label{e:prob2}
\end{align}
%The last term in \eqref{e:prob2}, $\Pr[ s^t = s\given x(k) = x]$, is defined in Section~\ref{s:learning algorithm} for the cases $m_i^x = C$ and $m_i^x = D$.
There are three cases for the transition probabilities in \eqref{e:prob2}. Before proceeding, we make the following observations. The last term in \eqref{e:prob2}, $\Pr[ s^t = \tilde{s}^t]$, is defined in Section~\ref{s:learning algorithm}; we will not repeat the definition here. For the first two terms, agents' state transition and strategy selection probabilities are independent when conditioned state $x$ and on the joint trial and acceptance strategy selections. Hence, we can write the first term as:
%We first examine $\Pr[ s^t = s\given x(k) = x]$ for an arbitrary joint strategy, $s = [s_1,s_2,\ldots,s_n]\in S.$ Because the trial strategy of each agent $i\in N$ in period $k$  depends only on its own state, $x_i(k) = [s_i^x, m_i^x]$, %and is independent of other agents' strategies,
%\begin{align*}
%\Pr[ s^t = s\given x(k) = x] = \prod_{i\in N}\Pr\bigl[s_i^t = s_i \given x_i(k) = [s_i^x,m_i^x]\bigr].
%\end{align*}
%From Section~\ref{s:learning algorithm}, if $m_i^x = C$, then
%\small
%\begin{equation*}
%\Pr\bigl[s_i^t = s_i \given x_i(k) = [s_i^x,C]\bigr] = \left\{
%\begin{array}{ll}
%1 - \eps^c &\text{if } s_i^t = s_i^x\\
%\eps^c\mathop{/}|\mathcal{A}_i| &\text{if } s_i^t = a_i,\\
%&\quad\quad\, \forall\,a_i\in \mathcal{A}_i\\
%0&\text{otherwise}
%\end{array}
%\right.
%\end{equation*}
%\normalsize
%If $m_i^x = D$, then 
%$$\Pr\bigl[s_i^t = s_i \given x_i(k) = [s_i^x,C]\bigr] = {1\over |S_i|},\,\forall\, s_i\in S_i.$$
%Middle Term
%In the term $\Pr[ s^a = s^y\given s^t = s,\,x(k) = x],$ each agent's acceptance strategy, $s_i^a$ depends on its current state, $x_i$, its trial strategy, $s_i^t$, and on the payoffs received in the evaluation and trial phases, $u_i^b$ and $u_i^t$.  Payoffs $u_i^b$ and $u_i^t$ depend on the joint baseline and joint trial strategies, $s^x$ and $s^t$.
%$\Pr[x^+\given s^t = \tilde{s}^t, s^a = s^+]$:
\begin{align}\label{e:first term}
\Pr[x^+\given s^t = \tilde{s}^t, s^a = s^+]= \prod_{i\in N}\Pr[x_i^+\given s^t = \tilde{s}^t, s^a = s^+]
\end{align}
and the second term as:
\begin{align}\label{e:second term}
\Pr[ s^a = s^+\given s^t = \tilde{s}^t] = \prod_{i\in N}\Pr[ s_i^a = s_i^+\given s^t = \tilde{s}^t].
\end{align}
The following three cases specify individual agents' probability of choosing the acceptance strategy $s_i^a$ in \eqref{e:second term} and transitioning to state $x_i^+$ in \eqref{e:first term}. %For convenience, define $U_i^{b+} := U_i(s^b)+\eps,U_i^{b-} := U_i(s^b) - \eps,$ and define $U_i^{t+}$ and $U_i^{t-}$ in the same way.
 %(i) agent $i$ is content, $m_i^x = C$, and did not experiment in the trial phase, (ii) agent $i$ is content, $m_i^x = C$,  and experimented during the trial phase, and (iii) agent $i$ is discontent, $m_i^x = D$.
 
\noindent\emph{Case (i) agent $i$ is content in state $x$, i.e., $m_i = C$, and did not experiment, $s_i^t = s_i$:}\\
\noindent For \eqref{e:second term}, since $s_i^a\in \{s_i^t,s_i\}$ we know that%the agent sets $s_i^a = s_i^t,$ regardless of its received payoffs in the evaluation and acceptance phases:
\begin{align*}
\Pr[ s_i^a = s_i^+\given s^t = \tilde{s}^t]
=\left\{
\begin{array}{ll}
1 &\text{if } s_i^+ = s_i\\
0 & \text{otherwise}
\end{array}
\right..
\end{align*}
In \eqref{e:first term}, for any trial strategy $s^t = \tilde{s}^t$, the probability of transitioning to a state $x_i^+$ depends on realized average payoffs $u_i^b$ and $u_i^a$. In particular, if $x_i^+  = [s_i^+,C]$, then we must have that $u_i^a\geq u_i^b - \eps$, so
\begin{align*}
&\Pr\biggl[x_i^+ = [s_i^+,C]\given s^a = s^+, s^t = \tilde{s}^t\biggr]\\
&\hspace{.05in} = \int_0^1 \Pr[u_i^b = \eta ] \int_{\eta-\eps}^1 \Pr[u_i^a = \nu \given s^t = \tilde{s}^t, s^a = s^+] d\nu d\eta.
\end{align*}
Then, the probability that $x_i^+ = [s_i^+,D]$ is
$$1 - \Pr\biggl[x_i^+ = [s_i^+,C]\given s^a = s^+, s^t = \tilde{s}^t\biggr].$$

%
\noindent\emph{Case (ii) agent $i$ is content and experimented, $s_i^t\neq s_i:$}\\
\noindent For \eqref{e:second term}, agent $i$'s acceptance strategy depends on its average baseline and trial payoffs, $u_i^b$ and $u_i^t$. Recall, if $u_i^t\geq u_i^b+\eps,$ then $s_i^a = s_i$, i.e., agent $i$'s acceptance strategy is simply its baseline strategy from state $x$. Otherwise $s_i^a = s_i^t.$
Utilities $u_i^b$ and $u_i^t$ depend on joint strategies $s$ and $s^t$ and on the common random signals sent during the corresponding phases. Therefore, 
% 
%\small
\begin{align*}
&\Pr[ s_i^a = s_i^+\given s^t = \tilde{s}^t\neq s]\nonumber\\
&\quad=\int_0^1\int_0^1 \Pr[ s_i^a = s_i^+\given u_i^b = \eta, u_i^t = \nu, s_i^t = s_i]\nonumber\\
&\hspace{.8in} \times \Pr[u_i^b = \eta]\Pr[u_i^t = \nu\given s^t = \tilde{s}^t]d\eta d\nu
\end{align*}
In \eqref{e:first term}, since agent $i$ remains content and sticks with its acceptance strategy from the previous period,
{\allowdisplaybreaks[3]\begin{align*}
&\Pr[x_i^+\given s^a = s^+, s^t = \tilde{s}^t]= \left\{
\begin{array}{ll}
1 & \text{if }s_i^+ = s_i^a\\
0 & \text{otherwise}
\end{array}
\right..
\end{align*}}
%\normalsize

\noindent\emph{Case (iii) agent $i$ is discontent:} \\
\noindent For \eqref{e:second term}, %agent $i$ sets $s_i^a = s_i^t$ regardless of its average payoffs in the evaluation and acceptance phases:
%\small
\begin{align*}
\Pr[ s_i^a = s_i^+\given s^t = \tilde{s}^t]%\nonumber\\
%&\quad 
= \left\{
\begin{array}{ll}
1 &\text{if } s_i^+ = s_i^t\\
0 &\text{otherwise}
\end{array}
\right. .
\end{align*}
%\normalsize
In \eqref{e:first term}, agent $i$'s probability of becoming content depends only on its received payoff during the acceptance phase; it becomes content with probability $\eps^{1 - u_i^a}$ and remains discontent with probability $1 - \eps^{1 - u_i^a}$. Hence, if $x_i^+ = [s_i^+,C]$,
\begin{align*}
&\Pr\biggl[x_i^+ = [s_i^+,C] \given s^a = s^+, s^t = \tilde{s}^t\biggr]\\
&\quad= \int_0^1 \eps^{1 - \eta}\Pr[u_i^a = \eta \given s^a = s^+, s^t = \tilde{s}^t]d\eta.
\end{align*}
Then, 
\begin{align*}
&\Pr\biggl[x_i^+ = [s_i^+,D] \given s^a = s^+, s^t = \tilde{s}^t\biggr] \\
&\quad= 1 - \Pr\biggl[x_i^+ = [s_i^+,C] \given s^a = s^+, s^t = \tilde{s}^t\biggr]
\end{align*}
%Likewise, if $m_i^y = D,$ 
%\begin{align*}
%&\Pr[x_i(k+1) = y_i\given s^t = s, s^a = s^y, x(k) = x)]\\
%&\quad = \int_0^1 (1 - \eps^{1 - \eta})\Pr[u_i^a = \eta \given s^a]d\eta.
%\end{align*}



Now that we have established transition probabilities for process $P^\eps$, we may state the following lemma.
\begin{lemma} \label{l:RPP}The process $P^\eps$ is a regular perturbation of $P^0.$ 
\end{lemma}

It is straightforward to see that $P^\eps$ satisfies the first two conditions of Definition~\ref{d:RPP} with respect to $P^0$. The fact that transition probabilities satisfy the third condition, Equation \eqref{e:RPP bounds}, follows from the fact that the dominant terms in $P^\eps_{x\to y}$ are polynomial in $\eps$. This is immediately clear in all but the incorporation of realized utilities into the transition probabilities, as in \eqref{e:prob2}. However, for any joint strategy, $s$, and associated average payoff $u_i$, since 
$$\E[u_i] = \E\left[{1\over\bar{p}}\sum_{\tau = \ell}^{\ell+\bar{p}-1} U_i(\s(z(\tau)))\right] = U_i(\s).  $$
for any time period of length $\bar{p}$ in which joint strategy $s$ is played throughout the entire period. Moreover,  
$\Var\bigl[U_i(\s(z(\tau)))\bigr] \leq 1.$ Therefore, we may use Chebyschev's inequality and the fact that  $\bar{p} = \lceil  1\mathbin{/} \eps^{nc+2}  \rceil$ to see that
\begin{equation}\label{e:old claim}
\Pr \Bigl[\bigl| u_i - U_i(\s)\bigr|\geq \eps\Bigr] \leq { \Var\bigl[U_i(\s(z(\tau)))\bigr]\over \bar{p} \eps^2}\leq \eps^{nc}.
\end{equation} 
Note that this applies for all average utilities, $u_i^b, u_i^t,$ and $u_i^a$ in the aforementioned state transition probabilities.
%and all other terms vanish in the limit when $r(x\to y)$ is chosen as the smallest power of $\eps$ in $P^\eps_{x\to y}$. The following claim bounds the size of any terms which may not be polynomial in $\eps$.

%\begin{claim}\label{l:averages}
%Let $\s^{b},\,\s^t,$ and $\s^a\in \S$ be the joint baseline, trial, and acceptance strategies played in period $k$ with corresponding average utilities $u_i^b,\,u_i^t,$ and $u_i^a$ as defined in \eqref{e:baseline payoff}, \eqref{e:trial payoff}, and \eqref{e:acceptance payoff}.  
%  Then
% $$\Pr\Bigl[u_i^\ell\notin [U_i(\s^\ell) \pm \eps]\Bigr] \leq \eps^{nc},\quad \ell\in \{b,t,a\}.$$
%\end{claim}
%
%\noindent\emph{Proof of Claim~\ref{l:averages}:}
%Since the signal $z(\tau)$ is chosen uniformly at random from $[0,1]$ for all $\tau\in\N$,
%$$\E\bigl[U_i(\s^\ell(z(\tau)))\bigr] = U_i(\s^\ell).  $$
%for any $\ell\in\{b,t,a\}.$
%Moreover, because $U_i(\tilde{a})\in[0,1]$ for all $\tilde{a}\in\mathcal{A},$ 
%$\Var\bigl[U_i(\s^\ell(z(\tau)))\bigr] \leq 1.$
%Then, using Chebyschev's inequality and the fact that  $\bar{p} = \lceil  1\mathbin{/} \eps^{nc+2}  \rceil$,
%$$\Pr \Bigl[\bigl| u_i - U_i(\s^\ell)\bigr|\geq \eps\Bigr] \leq { \Var\bigl[U_i(\s^\ell(z(\tau)))\bigr]\over \bar{p} \eps^2}\leq \eps^{nc}.$$
%\hfill\QED


%-------------------------- Recurrent classes --------------------------%
\vspace{.2cm}
\noindent \emph{Part 3:  Determining the stochastically stable states}
\vspace{.2cm}

%\subsection*{Recurrent classes}

%Recall $D^0\cup C^0$ is precisely the set of recurrent classes of the unperturbed process.  The set $D^0$ comprises a single recurrent class, since transitions between any two states in $D^0$ occur with positive probability in the unperturbed process.  Each state $y\in C^0$ comprises a single recurrent class since, for any $y\in C^0$, the system remains in state $y$ for all future time according to process $P^0.$

We begin by defining
\begin{align*}
&C^\star := \{x = \{[s_i,m_i]\}_{i\in N}\\
&\hspace{0.75in}\st q(s)\in \text{CCE} \text{ and } m_i = C,\,\forall i\in N\}\subseteq C^0
\end{align*} 
Here, we show that, if $C^\star$ is nonempty, then a state $x$ is stochastically stable if and only if $q(s)$  satisfies \eqref{e:SS1}. The fact that $q(s)$ must satisfy \eqref{e:SS2} when $C^\star = \emptyset$ follows in a similar manner. To accomplish this task, we (1) establish resistances between recurrent classes, and (2) compute stochastic potentials of each recurrent class.
%-------------------------- Edge Resistance Bounds --------------------------%
\subsection{Resistances between recurrent classes}

%Let $\s\in \S$ be the trial joint strategy  played during period $k\in \N.$  From Lemma~\ref{l:averages}, any transition which occurs when ${1\over p}\sum_{\ell=1}^p U_i(\s(z(t_\ell^k)))\notin [U_i(\s) - \eps,U_i(\s) + \eps]$ for at least one $i\in N$ occurs with probability at most $\eps^{nc},$ so has lower resistance $\lowerR(x,y) = nc$. 
%Hence, a path, $P$, which includes such a transition has lower resistance $\lowerR(P) \geq nc$.  

%Let $\s = (s_1^b,s_2^b,\ldots,s_n^b)$. For notational simplicity, define $U_i^+:= U_i(\s) + \eps$ and $U_i^-:= U_i(\s) - \eps.$ In the following, we state a series of claims about the resistances between recurrent classes. We provide a proof for Claim~\ref{c:r1} and omit proofs for the subsequent claims for brevity, since they follow in a similar manner.

We summarize resistances between recurrent classes in the following claim. 

%-----r(D->y)-----%
\begin{claim}\label{c:resistances}
 Resistances between recurrent classes satisfy:
%\begin{enumerate}%[leftmargin = .3cm]

\noindent For $x \in C^0$ with corresponding joint strategy $s$, 
\begin{equation}\label{e:D to x} r(D^0\to x) = \sum_{i\in N}(1 - U_i(\s)).\end{equation}

%\item 
%For $x\in C^0\setminus C^\star$, $y\in C^0,$ 
%$$r(x\to y)\geq c +\sum_{\stackrel{i\in N\st}{ U_i(s^y) < U_i(s^x)}}(1- U_i(s^y)).$$

%\item  The resistance of a sequence of transitions of the form $D^0\to x^0\to x^1\to\cdots\to x^m=x$, with  $x^k\in C^0\setminus C^\star$ for each $k<m$, and $x \in C^0,$ satisfies
%$$r(D^0\to x^0) + \sum_{k=1}^m r(x^{k-1}\to x^k)\geq \sum_{i\in N}(1-U_i(\s^x)) + mc.$$



\noindent For a transition of the form $x\to y$, where $x\in C^\star$ and $y \in (C^0\cup D^0)\setminus \{x\},$ 
\begin{equation}r(x\to y)\geq 2c.\end{equation}


\noindent For a transition of the form $x\to y$  where $x\in C^0\setminus C^\star$ and $y\in (C^0\cup D^0 )\setminus \{x\}$,
\begin{equation}r(x\to y)\geq c.\end{equation}


\noindent For every $x\in C^0\setminus C^\star$, there exists a path 
$x = x^0\to x^1\to\cdots\to x^m\in C^\star\cup D^0$ with resistance 
\begin{equation}r(x^j\to x^{j+1}) = c,\;\forall j\in \{0,1,\ldots,m-1\}.\end{equation}

\end{claim}

These resistances are computed in a similar manner to the proof establishing resistances in \cite{Marden2013c}; however, care must be taken due to the fact that there is a small probability that average received utilities fall outside of the window $U_i(s)\pm\eps$ during a phase in which joint strategy $s$ is played. We illustrate this by proving \eqref{e:D to x} in detail; the proofs are omitted for other types of transitions for brevity.

\begin{proof}
Let $x\in D^0$, $x^+\in C^0$ with $x_i = [s_i,D]$ and $x_i^+ = [s_i^+,C]$  for each $i\in N.$ Again, for notational brevity, we drop the dependence on state $x$ in the following probabilities. Note that all agents must select $s^t = s_i^+$ in order to transition to state $x_i = [s_i^+,C];$ otherwise the transition probability is 0.
%\begin{align*}
%&\Pr[x_i(k+1) = y\given s^t, s^a = s^y,x_i] \\
%&\quad := \Pr[x_i(k+1) = y\given s_i^t = s_i^y, s^a = s^y,x_i(k)=x_i]
%\end{align*}
we have
{\allowdisplaybreaks[3]\begin{align*}
&P^\eps_{x\to x^+} \nonumber\\
%&\quad = \sum_{s\in S}\Pr[x(k+1) = y\given s^t = s, s^a = s^y,x(k)=x]\nonumber\\
%&\hspace{1in} \times\Pr[ s^a = s^y\given s^t = s,\,x(k) = x]\nonumber\\
%&\hspace{1in} \times\Pr[ s^t = s^y\given x(k) = x]\\
&\quad \stackrel{(a)}{=}\Pr[x^+\given s^a = s^+, s^t = s^+]\nonumber\\
&\hspace{1in} \times\Pr[ s^a = s^+\given s^t = s^+]\Pr[ s^t = s^+]\\
&\quad \stackrel{(b)}{=}\Pr[x^+\given s^a = s^+, s^t = s^+]\Pr[ s^t = s^+]\\
&\quad \stackrel{(c)}{=}\Pr[x^+\given s^a = s^+, s^t = s^+]\prod_{i\in N}1\mathop{/}|S_i|\\
%&\quad =\prod_{i\in N} \Pr[x_i(k+1) = y\given s^t, s^a = s^y,x_i]\prod_{i\in N}1\mathop{/}|S_i|\\
&\quad = \prod_{i\in N}{1\over|S_i|}\Pr[x_i^+\given s^a = s^+,s^t = s^+]
\end{align*}}
where:
%\begin{enumerate*}[(a)]
(a) follows from the fact that $s_i^a = s_i^t$ since $m_i = D$ in state $x$ for all $i\in N$,
(b) $\Pr[ s^a = s^+\given s^t = s^+] = 1$ since all agents are discontent and hence commit to their trial strategies during the acceptance period, and
(c) $\Pr[ s^t = s^+] = \prod_{i\in N}1\mathop{/}|S_i|$ since each discontent agent selects its trial strategy uniformly at random from $S_i$.


We now show that
\begin{equation}
0 < \lim_{\eps\to 0^+} \frac{P^\eps_{x\to x^+}}{\eps^{\sum_{i\in N}1 - U_i(s^+)}}<\infty
\end{equation}
satisfying \eqref{e:RPP bounds}. For notational simplicity, we define 
\begin{align}
U_i^+	&:= U_i(s^+)+\eps,\nonumber\\
U_i^-	&:=U_i(s^+) - \eps.\label{e:u defns}
\end{align}
We first lower bound $P^\eps_{x\to x^+} :$
\small
{\allowdisplaybreaks[3]\begin{align}
&P^\eps_{x\to x^+}	\nonumber\\
			&\quad= \prod_{i\in N}{1\over|S_i|}\Pr[x_i^+ \given s^a = s^+, s^t =s^+]\nonumber\\
			&\quad= \prod_{i\in N}{1\over |S_i|}\int_0^1 \Pr[u_i^a = \eta \given s^a = s^+, s^t = s^+]\eps^{1-\eta}d\eta\nonumber\\
			%&\quad= \prod_{i\in N}{1\over |S_i|}\left(\int_0^{U_i^-} \Pr[u_i^a = \eta\given s^t, s^a = s^+]\eps^{1-\eta}d\eta \right.\\
			%&\hspace{.7in} + \int_{U_i^-}^{U_i^+}\Pr[u_i^a = \eta\given s^t, s^a = s^y,x_i]\eps^{1-\eta}d\eta \\
			%&\hspace{.7in} + \left.\int_{U_i^+}^1\Pr[u_i^a = \eta\given s^t, s^a = s^y,x_i]\eps^{1-\eta}d\eta\right)\\	
			&\quad\geq \prod_{i\in N}{1\over |S_i|}\int_{U_i^-}^{U_i^+}\Pr[u_i^a = \eta\given s^a = s^+, s^t = s^+ ]\eps^{1-\eta}d\eta\nonumber\\
			&\quad\stackrel{(a)}{\geq}\prod_{i\in N}{\eps^{1 - U_i^-}\over |S_i|}\int_{U_i^-}^{U_i^+} \Pr[u_i^a = \eta\given  s^a = s^+, s^t = s^+] d\eta\nonumber\\
			&\quad\stackrel{(b)}{\geq}\prod_{i\in N}{\eps^{1 - U_i^-}\over |S_i|}(1-\eps^{nc})\nonumber\\
			%&\quad=\prod_{i\in N}{\eps^{1 - U_i^-} - \eps^{1 - U_i^+ +nc}\over|S_i|}\\
			&\quad={\eps^{\sum_{i\in N} 1 - U_i^-} + O(\eps^{nc})\over\prod_{i\in N} |S_i|}\label{e:lb}% \red{am i using this notation correctly?},
\end{align}}
\normalsize
where
%\begin{enumerate}[(a)]
(a) is from the fact that $\eps^{1-\eta}$ is continuous and increasing in $\eta$ for $\eps\in(0,1),$ and 
(b) follows from \eqref{e:old claim}.  
%Then, 
%\small
%\begin{align}
%&\lim_{\eps\to 0^+} {P^\eps_{x\to y}\over \eps^{\left(\sum_{i\in N} 1 - U_i(\s^y)  \right)} }\nonumber\\
%	&\quad\quad\geq \left(\prod_{i\in N}{1\over |S_i|}\right)\lim_{\eps\to 0^+}{\eps^{\sum_{i\in N} 1 - U_i^+} +O(\eps^{nc})\over \eps^{\left(\sum_{i\in N} 1 - U_i(\s^y)  \right)} } \nonumber\\
%	&\quad\quad= \prod_{i\in N}{1\over |S_i|}>0 \label{e:lb}
%\end{align}
%\normalsize
%as desired.
Continuing in a similar fashion, it is straightforward to show 
\begin{equation}\label{e:ub}
P^\eps_{x\to x^+}\leq\eps^{\sum_{i\in N}(1-U_i^+)} + O(\eps^{nc}).
\end{equation}

%\red{start}
%\small
%{\allowdisplaybreaks[3]\begin{align*}
%&P^\eps_{x\to y}\\
%			&\quad= \prod_{i\in N}{1\over |S_i|}\int_0^1 \Pr[u_i^a = \eta\given s^t, s^a = s^y,x_i]\eps^{1-\eta}d\eta\\
%			&\quad= \prod_{i\in N}{1\over |S_i|}\left(\int_0^{U_i^-} \Pr[u_i^a = \eta\given s^t, s^a = s^y,x_i]\eps^{1-\eta}d\eta \right.\\
%			&\hspace{.7in} + \int_{U_i^-}^{U_i^+}\Pr[u_i^a = \eta\given s^t, s^a = s^y,x_i]\eps^{1-\eta}d\eta  \\
%			&\hspace{.7in}+ \left.\int_{U_i^+}^1\Pr[u_i^a = \eta\given s^t, s^a = s^y,x_i]\eps^{1-\eta}d\eta\right)\\	
%			&\quad\leq \prod_{i \in N}\left(\int_0^{U_i^-}\Pr[u_i^a = \eta\given s^t, s^a = s^y,x_i]d\eta
%			\right.\\
%			&\hspace{.7in}+ \eps^{1 - U_i^+}\int_	{U_i^-}^{U_i^+}\Pr[u_i^a = \eta\given s^t, s^a = s^y,x_i]d\eta \\
%			&\hspace{.7in}+ \left.\int_{U_i^+}^1\Pr[u_i^a = \eta\given s^t, s^a = s^y,x_i]d\eta	\right)\\
%			&\quad\leq\prod_{i \in N}\left( \eps^{nc} + \eps^{1-U_i^+}\int_	{U_i^-}^{U_i^+}\Pr[u_i^a = \eta\given s^t, s^a = s^y,x_i]d\eta\right)\\
%			&\quad\leq \prod_{i \in N}\left(\eps^{nc} + \eps^{1-U_i^+}\right)\\
%			&\quad=\eps^{\sum_{i\in N}(1-U_i^+)} + O(\eps^{nc})
%\end{align*}}
%\normalsize
%Then,
%\small
%\begin{align}
%&\lim_{\eps\to 0^+}{\Pr[x_{N_1}\to y_{N_1}\given x_{N\setminus N_1}\to y_{N\setminus N_1}]\over \eps^{\left(\sum_{i\in N_1} 1 - U_i(\s^y)  \right)} }\nonumber\\
%&\quad\quad\leq	\lim_{\eps\to 0^+}{\eps^{\sum_{i\in N_1}1 - U_i^+} + O(\eps^{nc})\over\eps^{\left(\sum_{i\in N_1} 1 - U_i(\s^y)  \right)} }  \nonumber\\
%&\quad\quad= 1<\infty.\label{e:ub}
%\end{align}
%\normalsize
%Therefore, the transition probability, $P^\eps_{x\to y}$ satisfies \eqref{e:RPP bounds} with resistance $r(x\to y) = \sum_{i\in N}\left(1 - U_i(s^y)\right).$ 
%
%\red{end}

Given \eqref{e:lb} and \eqref{e:ub}, and the fact that $U_i^+$ and $U_i^-$ satisfy \eqref{e:u defns}, we have that $P_{x\to x^+}^\eps$ satisfies \eqref{e:RPP bounds} with resistance $\sum_{i\in N}\left(1 - U_i(s^+)\right)$ as desired.
\end{proof}



%\noindent\emph{Proof:}
%To lower bound the resistance of a (multi-step) transition from $y\in C^0$ to $\tilde{y}\in C^0,$ we examine the easiest possible way for this transition to occur. Note that there are paths which require additional steps, or involve more agents experimenting, but these paths occur with higher resistance than the one outlined below.

%First note that, in order to transition from $y\in C^0$ to $\tilde{y}\in C^0,$ at least one agent must experiment with an alternate strategy, which occurs with resistance $c$. Any agent whose utility increases as a result of this experimentation becomes content with the new joint strategy within two periods. Agents whose utilities decrease become discontent. By a similar argument to the proof of Claim~\ref{c:r1} each of these agents becomes content with the new joint strategy with a resistance of $1 - U_i(\tilde{s})$. Since this is the easiest, i.e., highest probability, way to transition from $y$ to $\tilde{y}$, the total resistance of any transition from $C^0\setminus C^\star$ to $C^\star$ is at least $c +\sum_{i\in N\st U_i(\tilde{s}) < U_i(\s)}(1- U_i(\tilde{s}))$.
%\hfill\QED




%\noindent\emph{Proof:}
%This follows from the facts that:
%(i) For each transition $y^j\to y^{j+1}$, some agent must experiment with an alternate strategy, which occurs with resistance $c$, and 
%(ii) For each agent $i\in N$, either: \\
%\noindent\textbullet \; During the transition $D\to y^0,$ agent $i$ accepts a payoff for a joint strategy which is less than or equal to the payoff $U_i(\s)$, which occurs with resistance at least $1 - U_i(\s)$, OR\\
%\noindent\textbullet \; For some transition $y^j\to y^{j+1},$ agent $i$'s payoff decreases to a payoff less than or equal to $U_i(\s)$ as a result of an agent's experimentation. Agent $i$ accepts this new joint action with resistance at least $1 - U_i(\s).$
%\hfill\QED


%\noindent\emph{Proof:}
%First note that, from state $y\in C^\star$, if a single agent $i\in N$ experiments with an alternate trial strategy, its payoff decreases. Hence, agent $i$ will reject that trial strategy, and return to its previous baseline strategy, causing all other agents to return to their previous content states. To transition to state $\tilde{y}$ at least two agents must experiment with alternate trial strategies at the same time, which occurs with a resistance of at least $2c$.
%\hfill\QED


%\noindent\emph{Proof:}
%The highest probability way to transition from a state in $C^0$ to $D$ is for a single agent to experiment and accept the new trial action as its baseline. If the experimentation causes another agent's payoff to drop, that agent will then become discontent within two periods, causing all other agents to become discontent with resistance zero.
%\hfill\QED


%\noindent\emph{Proof:}
%The highest probability way to transition from a state in $C^\star$ to a state in $D$ is for a single agent to become discontent, which occurs with a resistance of $2c$. All other agents subsequently become discontent with a resistance of zero. Alternately, two agents may experiment with and accept new trial strategies, which happens with resistance $2c$, causing some other agent to become discontent. Then all other agents become discontent with zero resistance. 

%Note again that if a single agent experiments from a state in $C^\star$, its payoff will drop and it will not accept the new trial strategy.
%\hfill\QED


%\noindent\emph{Proof:}
%Let $y\in C^0\setminus C^\star$ with associated joint baseline strategy $\s = (s_i^b,s_{-i}^b).$ 
%Since $y\notin C^\star,$ there exists $i\in N$ and strategy $s_i^\prime = a_i^\prime$ for some $a_i\in\mathcal{A}_i$ such that $U_i(s_i^\prime,s_{-i}^b) > U_i(\s).$ If agent $i$ experiments with $s_i = a_i,$ it will become content with the trial strategy $s_i^\prime$ as its new baseline. This experimentation occurs with resistance $c$. If any other agent's payoff decreases as a result of agent $i$'s experimentation, that agent becomes discontent, leading all other agents to become discontent with resistance zero, and we are done. Hence we may assume that all agents' payoffs weakly increase as a result of agent $i$'s experimentation, so that the sum of agents' expected payoffs, $\sum_{i\in N} U_i(\s)$ strictly increases. 

%If the result is a state in $C^0\setminus C^\star$, we may repeat the analysis. The sum of expected payoffs is uniquely determined by the state, the set $C^0\setminus C^\star$ is finite, and there exists a resistance $c$ transition to a state in either $D$ or a state in $C^0$ for all states in $C^0\setminus C^\star$. Therefore this process must eventually terminate with a resistance $c$ transition to $D$ or $C^\star$ as desired. %\red{this is messy i feel like I should be able to make is much shorter!}
%\hfill\QED

\subsection{Stochastic potentials}

The following lemma specifies stochastic potentials of each recurrent class. Using resistances from Claim~\ref{c:resistances}, the stochastic potentials follow from the same arguments as in \cite{Marden2013c}. The proof is repeated below for completeness.%; we omit the proof for brevity.

\begin{lemma}\label{l:sps}
Let $x\in C^0\setminus C^\star$ with corresponding joint strategy $s$, and let $x^\star\in C^\star$ with corresponding joint strategy $s^\star.$ The stochastic potentials of each recurrent class are:
\begin{align*}
\gamma(D^0) &= c|C^0\setminus C^\star| + 2c|C^\star|,\\
\gamma(x) &= \left(|C^0\setminus C^\star| - 1\right)c + 2c|C^\star| + \sum_{i\in N}(1 - U_i(\s)),\\
%&\hspace{2in} x\in C^0\setminus C^\star\\
\gamma(x^\star) &= |C^0\setminus C^\star|c + 2c\left(|C^\star|-1\right) + \sum_{i\in N}(1 - U_i(\s^\star)),%\\
%& \hspace{2in} x^\star\in C^\star.
\end{align*}
\end{lemma}

\emph{Proof:}
In order to establish the stochastic potentials for each recurrent class, we will lower and upper bound them.

%-------------------------- SP lower bounds --------------------------%
\noindent\emph{Lower bounding the stochastic potentials}: To lower bound the stochastic potentials of each recurrent class, we determine the lowest possible resistance that a tree rooted at each of these classes may have.

\smallskip
%\begin{itemize}[leftmargin = .3cm]
\noindent 1) Lower bounding $\gamma(D^0)$:
$$\gamma(D^0) \geq c|C^0\setminus C^\star| + 2c|C^\star|$$ 
In a tree rooted at $D^0$, each state in $C^0$ must have an exiting edge. In order to exit a state in $C^0\setminus C^\star$, only a single agent must experiment, contributing resistance $c$. To exit a state in $C^\star$, at least two agents must experiment, contributing resistance $2c.$

\smallskip

\noindent 2) Lower bounding $\gamma(x)$, $x\in C^0\setminus C^\star$:
$$\gamma(x) \geq \left(|C^0\setminus C^\star| - 1\right)c + 2c|C^\star| + \sum_{i\in N}(1 - U_i(\s))$$ 
 Here, each state in $C^0\setminus \{x\}$ must have an exiting edge, which contributes resistance $\left(|C^0\setminus C^\star| - 1\right)c + 2c|C^\star|.$ The recurrent class $D^0$ must also have an exiting edge, contributing at least resistance $\sum_{i\in N}(1 - U_i(\s)).$

\smallskip

\noindent 3) Lower bounding $\gamma(x^\star)$, $x^\star\in C^\star$:
$$\gamma(x^\star) \geq |C^0\setminus C^\star|c + 2c\left(|C^\star|-1\right) + \sum_{i\in N}(1 - U_i(\s^\star))$$ 
 Again, each state in $C^0\setminus \{x^\star\}$ must have an exiting edge, which contributes resistance $\left(|C^0\setminus C^\star| - 1\right)c + 2c|C^\star|.$ The recurrent class $D^0$ must also have an exiting edge, contributing resistance at least $\sum_{i\in N}(1 - U_i(\s^\star)).$

\smallskip

\noindent\emph{Upper bounding the stochastic potentials:} In order to upper bound the stochastic potentials, we construct trees rooted at each recurrent class which have precisely the resistances established above.

\smallskip
%\begin{itemize}[leftmargin = .3cm]
\noindent 1) Upper bounding $\gamma(D^0)$:
$$\gamma(D^0) \leq c|C^0\setminus C^\star| + 2c|C^\star|$$ 
Begin with an empty graph with vertices $X$. For each state $x\in C^0\setminus C^\star$, add a path ending in $C^\star\cup D^0$ so that each edge has resistance $c$. This is possible due to Claim~\ref{c:resistances}. Now eliminate redundant edges; this contributes resistance at most $c|C^0\setminus C^\star|$ since each state in $C^0\setminus C^\star$ has exactly one outgoing edge. Finally, add an edge $x^\star \to D^0$ for each $x^\star\in C^0;$ this contributes resistance $2c|C^\star|$.

\smallskip

\noindent 2) Upper bounding $\gamma(x)$, $x\in C^0\setminus C^\star$:
$$\gamma(x) \leq \left(|C^0\setminus C^\star| - 1\right)c + 2c|C^\star| + \sum_{i\in N}(1 - U_i(\s)),$$ 
 This follows by a similar argument to the previous upper bound, except here we add an edge $D^0 \to x$ which contributes resistance $\sum_{i\in N}(1 - U_i(\s))$.

\smallskip

\noindent 3) Upper bounding $\gamma(x^\star)$, $x^\star\in C^\star:$
$$\gamma(x^\star) \leq |C^0\setminus C^\star|c + 2c\left(|C^\star|-1\right) + \sum_{i\in N}(1 - U_i(\s^\star)),$$ 
 This follows from an identical argument to the previous bound.
%\end{itemize}
\hfill\qed




%-------------------------- SP upper bounds --------------------------%
%\subsubsection*{Upper bounding the stochastic potentials}

%Here we show that the stochastic potentials of states in $C^0\cup D^0$ are precisely equal to the lower bounds established above. We do this by establishing upper bounds via tree construction arguments. 


%\begin{lemma}\label{l:spC0}
% For a state $y\in C^0\setminus C^\star$,
%$$\gamma(y)= \left(|C^0\setminus C^\star| - 1\right)c + 2c|C^\star| + \sum_{i\in N}(1 - U_i(\s))$$
%\end{lemma}

%\noindent\emph{Proof:}
%Let $y\in C^0\setminus C^\star$ with $y_i = [s_i^b,\s,C].$  We will construct a tree rooted at $y$ on the recurrent classes.  
%\begin{itemize}[leftmargin = .3cm]
%\item Add edge $D\to y$ which has resistance $\sum_{i\in N}(1-U_i(\s))$. 
%\item For each state $\tilde{y} \in C^0\setminus C^\star$, add a path from $\tilde{y}$ to $C^\star\cup D$ such that each edge along the path has resistance $c$ (this is possible due to Lemma~\ref{l:path}) and eliminate redundant edges. This adds total resistance at most $c(|C^0\setminus C^\star| - 1),$ since it results in adding exactly one outgoing edge from each state in $C^0\setminus (C^\star\cup\{x\})$.  
%\item Finally, add an edge $\tilde{y}\to D$, which has resistance $2c$, for each $\tilde{y} \in C^\star$. This step adds resistance $2c|C^\star|.$  
%\end{itemize}
%We have constructed a tree rooted at $y\in C^0\setminus C^\star$ with total resistance $\left(|C^0\setminus C^\star| - 1\right)c + 2c|C^\star| + \sum_{i\in N}(1 - U_i(\s)),$ upper bounding the stochastic potential of $y$. Combining this upper bound with the lower bound of Lemma~\ref{l:lbC0} establishes Lemma~\ref{l:spC0}.  
%\hfill\QED


%\begin{lemma}\label{l:spCstar}
%For a state $y\in C^\star,$
%$$\gamma(y)= |C^0\setminus C^\star|c + 2c\left(|C^\star|-1\right) + \sum_{i\in N}(1 - U_i(\s))$$
%\end{lemma}
%\noindent\emph{Proof:}
%Lemma~\ref{l:spCstar} follows via a similar argument as the proof for Lemma~\ref{l:spC0}.
%\hfill\QED

%\begin{lemma}\label{l:spD}
%The class $D^0$ has stochastic potential
%$$\gamma(D^0)= c|C^0\setminus C^\star| + 2c|C^\star|.$$
%\end{lemma}
%\noindent\emph{Proof:}
%Lemma~\ref{l:spD} follows via a similar argument as the proofs for the previous two lemmas. 
%\hfill\QED

We now use Lemma~\ref{l:sps} to complete the proof of Theorem~\ref{t:main theorem}. For the first part, suppose $C^\star$ is nonempty, and let $$x^\star\in \argmax_{x\in C^\star} \sum{U_i(\s)},$$ where joint strategy $s$ corresponds to state $x$.  Then,
\begin{align*}
\gamma(x^\star)  	&=  |C^0\setminus C^\star|c + 2c\left(|C^\star|-1\right) + \sum_{i\in N}(1 - U_i(\s^*))\\
			&<  |C^0\setminus C^\star|c + 2c|C^\star|  \quad\text{(since $c\geq n$)} \\
			&= \gamma(D).
\end{align*}
For $x \in C^0$, 
\begin{align*}
\gamma(x^\star)  	&=  |C^0\setminus C^\star|c + 2c\left(|C^\star|-1\right) + \sum_{i\in N}(1 - U_i(\s^\star))\\
			&<   |C^0\setminus C^\star - 1|c + 2c\left(|C^\star|\right) + \sum_{i\in N}(1 - U_i(s)) \\
			&=\gamma(x).
\end{align*}
For $x\in C^\star$ with $$x\notin \argmax_{x\in C^\star} \sum{U_i(\s)},$$
\begin{align*}
\gamma(x^\star)  	&=  |C^0\setminus C^\star|c + 2c\left(|C^\star|-1\right) + \sum_{i\in N}(1 - U_i(\s^\star))\\
				&<  |C^0\setminus C^\star|c + 2c\left(|C^\star|-1\right) + \sum_{i\in N}(1 - U_i(s)\\
				&=\gamma(x).
\end{align*}
Applying Theorem~\ref{t:Young Theorem}, $x^\star$ is stochastically stable. Since all other states have strictly larger stochastic potential, \emph{only} states $x^\star\in C^\star$ with $x^\star\in \argmax_{x\in C^\star} \sum{U_i(\s)}$ are stochastically stable. From state $x^\star$, if each agent plays according to its baseline strategy, then the probability that joint action $a\in \mathcal{A}$ is played at any given time is $\Pr(a  = a^\prime) = q^{a^\prime(s^\star)}.$  This implies that a CCE which maximizes the sum of agents' payoffs is played with high probability as $\eps\to 0,$ after sufficient time has passed.

The second part of the theorem follows similarly by considering the case when $C^\star = \emptyset.$ \hfill\qed



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
